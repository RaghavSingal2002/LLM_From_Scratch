{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11660612,"sourceType":"datasetVersion","datasetId":7317700}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Chapter - 1**\n\n\n\n*   There is no code in this chapter.\n\n","metadata":{"id":"YS-dyYuuqb4V"}},{"cell_type":"markdown","source":"\n\n---\n\n\n\n---\n\n","metadata":{"id":"BU98eHAJDDCs"}},{"cell_type":"markdown","source":"**Chapter - 2 Working with Text Data**","metadata":{"id":"Xj4SJxYzoIA0"}},{"cell_type":"code","source":"pip install torch tiktoken\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KM9hD_brlvui","outputId":"37dc9cae-fe82-4b60-cd21-f2df68646d42","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:17:45.584324Z","iopub.execute_input":"2025-05-03T18:17:45.584590Z","iopub.status.idle":"2025-05-03T18:18:57.234963Z","shell.execute_reply.started":"2025-05-03T18:17:45.584569Z","shell.execute_reply":"2025-05-03T18:18:57.234027Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import importlib.metadata\n\nfor package in [\"torch\", \"tiktoken\"]:\n    try:\n        print(f\"{package} version:\", importlib.metadata.version(package))\n    except importlib.metadata.PackageNotFoundError:\n        print(f\"{package} is not installed.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wT_eeT9FmOSN","outputId":"0e87ca6c-144d-4cf9-dfbc-594dbe996b7c","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.236463Z","iopub.execute_input":"2025-05-03T18:18:57.236701Z","iopub.status.idle":"2025-05-03T18:18:57.252990Z","shell.execute_reply.started":"2025-05-03T18:18:57.236682Z","shell.execute_reply":"2025-05-03T18:18:57.252375Z"}},"outputs":[{"name":"stdout","text":"torch version: 2.5.1+cu124\ntiktoken version: 0.9.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**2.1 Understanding word embeddings**\n\n\n*   LLMs work with embeddings in high-dimensional spaces (i.e., thousands of dimensions)\n\nSince we can't visualize such high-dimensional spaces (we\n\n*   humans think in 1, 2, or 3 dimensions), the figure below illustrates a 2-dimensional embedding space  \n\n","metadata":{"id":"0YXsx-prp_tG"}},{"cell_type":"markdown","source":"**2.2 Tokenizing text**","metadata":{"id":"F_4cMVgtoVsY"}},{"cell_type":"code","source":"import os\nimport urllib.request\n\nif not os.path.exists(\"the-verdict.txt\"):\n    url = (\"https://raw.githubusercontent.com/rasbt/\"\n           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n           \"the-verdict.txt\")\n    file_path = \"the-verdict.txt\"\n    urllib.request.urlretrieve(url, file_path)","metadata":{"id":"Afa7AU4roB2A","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.253788Z","iopub.execute_input":"2025-05-03T18:18:57.254034Z","iopub.status.idle":"2025-05-03T18:18:57.447794Z","shell.execute_reply.started":"2025-05-03T18:18:57.254010Z","shell.execute_reply":"2025-05-03T18:18:57.447256Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\n\nprint(\"Total number of character:\", len(raw_text))\nprint(raw_text[:99])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpDs6E87oW0m","outputId":"119270c3-b433-4af6-bd9c-948eb06062f2","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.448519Z","iopub.execute_input":"2025-05-03T18:18:57.448759Z","iopub.status.idle":"2025-05-03T18:18:57.453406Z","shell.execute_reply.started":"2025-05-03T18:18:57.448732Z","shell.execute_reply":"2025-05-03T18:18:57.452853Z"}},"outputs":[{"name":"stdout","text":"Total number of character: 20479\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"\n\n*  The goal is to tokenize and embed this text for an LLM.\n\n\n*   Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above\n*   The following regular expression will split on whitespaces\n\n","metadata":{"id":"BILv-4AioiFy"}},{"cell_type":"code","source":"import re\n\ntext = \"Hello, world. This, is a test.\"\nresult = re.split(r'(\\s)', text)\n\nprint(result)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rHR70ryYoeQ4","outputId":"d13cec1f-60cd-4220-e327-66cd799828a1","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.456911Z","iopub.execute_input":"2025-05-03T18:18:57.457101Z","iopub.status.idle":"2025-05-03T18:18:57.468219Z","shell.execute_reply.started":"2025-05-03T18:18:57.457085Z","shell.execute_reply":"2025-05-03T18:18:57.467631Z"}},"outputs":[{"name":"stdout","text":"['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"\n\n*   We don't only want to split on whitespaces but also commas and periods, so let's modify the regular expression to do that as well\n\n","metadata":{"id":"Vur3zbu1pGJQ"}},{"cell_type":"code","source":"result = re.split(r'([,.]|\\s)', text)\n\nprint(result)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZLNXym2EpDrC","outputId":"b835c75d-cc3b-4abd-cc84-7a6421195dea","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.468845Z","iopub.execute_input":"2025-05-03T18:18:57.469135Z","iopub.status.idle":"2025-05-03T18:18:57.482564Z","shell.execute_reply.started":"2025-05-03T18:18:57.469095Z","shell.execute_reply":"2025-05-03T18:18:57.482019Z"}},"outputs":[{"name":"stdout","text":"['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"\n\n*   As we can see, this creates empty strings, let's remove them\n\n","metadata":{"id":"7-dgjp74pR9U"}},{"cell_type":"code","source":"# Strip whitespace from each item and then filter out any empty strings.\nresult = [item for item in result if item.strip()]\nprint(result)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhtmQBxlpG7R","outputId":"3eaa9cb3-15ae-432b-996b-5643507afd11","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.483250Z","iopub.execute_input":"2025-05-03T18:18:57.483539Z","iopub.status.idle":"2025-05-03T18:18:57.498582Z","shell.execute_reply.started":"2025-05-03T18:18:57.483522Z","shell.execute_reply":"2025-05-03T18:18:57.497893Z"}},"outputs":[{"name":"stdout","text":"['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"\n\n*   This looks pretty good, but let's also handle other types of punctuation, such as periods, question marks, and so on\n\n","metadata":{"id":"NWrsVmTvpg4k"}},{"cell_type":"code","source":"text = \"Hello, world. Is this-- a test?\"\n\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9WOVdvlmplEK","outputId":"75d828d4-3e4a-4412-ed21-d65221284821","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.499333Z","iopub.execute_input":"2025-05-03T18:18:57.499555Z","iopub.status.idle":"2025-05-03T18:18:57.515430Z","shell.execute_reply.started":"2025-05-03T18:18:57.499535Z","shell.execute_reply":"2025-05-03T18:18:57.514786Z"}},"outputs":[{"name":"stdout","text":"['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"\n\n*   This is pretty good, and we are now ready to apply this tokenization to the raw text\n\n","metadata":{"id":"OnYqLLtMpquv"}},{"cell_type":"code","source":"preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(preprocessed[:30])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2HO3e6Xqptr2","outputId":"6485fc6a-3aa1-4b8b-fcc4-e3098351bd55","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.516072Z","iopub.execute_input":"2025-05-03T18:18:57.516237Z","iopub.status.idle":"2025-05-03T18:18:57.532362Z","shell.execute_reply.started":"2025-05-03T18:18:57.516224Z","shell.execute_reply":"2025-05-03T18:18:57.531631Z"}},"outputs":[{"name":"stdout","text":"['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"\n\n*   Let's calculate the total number of tokens\n\n","metadata":{"id":"lTDRL_rypxVd"}},{"cell_type":"code","source":"print(len(preprocessed))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13-Jivrbpwje","outputId":"002ec80b-e813-4df5-df46-e122f862c04f","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.533278Z","iopub.execute_input":"2025-05-03T18:18:57.533527Z","iopub.status.idle":"2025-05-03T18:18:57.545910Z","shell.execute_reply.started":"2025-05-03T18:18:57.533511Z","shell.execute_reply":"2025-05-03T18:18:57.545338Z"}},"outputs":[{"name":"stdout","text":"4690\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**2.3 Converting tokens into token IDs**\n\n\n\n*   Next, we convert the text tokens into token IDs that we can process via embedding layers later\n\n","metadata":{"id":"MG7mxd4krJLi"}},{"cell_type":"code","source":"all_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\n\nprint(vocab_size)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MPcWoGKBp4dg","outputId":"ba290b16-91b0-477f-bf57-686c359eb459","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.546546Z","iopub.execute_input":"2025-05-03T18:18:57.546726Z","iopub.status.idle":"2025-05-03T18:18:57.561986Z","shell.execute_reply.started":"2025-05-03T18:18:57.546712Z","shell.execute_reply":"2025-05-03T18:18:57.561294Z"}},"outputs":[{"name":"stdout","text":"1130\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"vocab = {token:integer for integer,token in enumerate(all_words)}","metadata":{"id":"q9fNp_qxrXTW","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.562810Z","iopub.execute_input":"2025-05-03T18:18:57.563048Z","iopub.status.idle":"2025-05-03T18:18:57.575612Z","shell.execute_reply.started":"2025-05-03T18:18:57.563024Z","shell.execute_reply":"2025-05-03T18:18:57.574995Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"\n\n*   Below are the first 50 entries in this vocabulary:\n","metadata":{"id":"HOLA2sH9raQZ"}},{"cell_type":"code","source":"for i, item in enumerate(vocab.items()):\n    print(item)\n    if i >= 50:\n        break","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LiL4XKg4rZNo","outputId":"123d8275-4d1f-4bf7-eebe-dbab326ca409","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.576382Z","iopub.execute_input":"2025-05-03T18:18:57.576896Z","iopub.status.idle":"2025-05-03T18:18:57.591696Z","shell.execute_reply.started":"2025-05-03T18:18:57.576878Z","shell.execute_reply":"2025-05-03T18:18:57.590977Z"}},"outputs":[{"name":"stdout","text":"('!', 0)\n('\"', 1)\n(\"'\", 2)\n('(', 3)\n(')', 4)\n(',', 5)\n('--', 6)\n('.', 7)\n(':', 8)\n(';', 9)\n('?', 10)\n('A', 11)\n('Ah', 12)\n('Among', 13)\n('And', 14)\n('Are', 15)\n('Arrt', 16)\n('As', 17)\n('At', 18)\n('Be', 19)\n('Begin', 20)\n('Burlington', 21)\n('But', 22)\n('By', 23)\n('Carlo', 24)\n('Chicago', 25)\n('Claude', 26)\n('Come', 27)\n('Croft', 28)\n('Destroyed', 29)\n('Devonshire', 30)\n('Don', 31)\n('Dubarry', 32)\n('Emperors', 33)\n('Florence', 34)\n('For', 35)\n('Gallery', 36)\n('Gideon', 37)\n('Gisburn', 38)\n('Gisburns', 39)\n('Grafton', 40)\n('Greek', 41)\n('Grindle', 42)\n('Grindles', 43)\n('HAD', 44)\n('Had', 45)\n('Hang', 46)\n('Has', 47)\n('He', 48)\n('Her', 49)\n('Hermia', 50)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"\n\n*   Below, we illustrate the tokenization of a short sample text using a small vocabulary:\n\n*   Putting it now all together into a tokenizer class\n\n\n\n","metadata":{"id":"qNlz1IzxrmLi"}},{"cell_type":"code","source":"class SimpleTokenizerV1:\n    def __init__(self, vocab):\n        self.str_to_int = vocab\n        self.int_to_str = {i:s for s,i in vocab.items()}\n\n    def encode(self, text):\n        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n\n        preprocessed = [\n            item.strip() for item in preprocessed if item.strip()\n        ]\n        ids = [self.str_to_int[s] for s in preprocessed]\n        return ids\n\n    def decode(self, ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        # Replace spaces before the specified punctuations\n        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n        return text","metadata":{"id":"GHj117HBrg3D","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.594753Z","iopub.execute_input":"2025-05-03T18:18:57.595014Z","iopub.status.idle":"2025-05-03T18:18:57.605807Z","shell.execute_reply.started":"2025-05-03T18:18:57.594999Z","shell.execute_reply":"2025-05-03T18:18:57.605272Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"\n\n*   The encode function turns text into token IDs.\n\n\n  \n*  The decode function turns token IDs back into text.\n\n","metadata":{"id":"uFn0iOI6r05h"}},{"cell_type":"code","source":"tokenizer = SimpleTokenizerV1(vocab)\n\ntext = \"\"\"\"It's the last he painted, you know,\"\n           Mrs. Gisburn said with pardonable pride.\"\"\"\nids = tokenizer.encode(text)\nprint(ids)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VgFKyEuCrpj2","outputId":"181fec02-2bc3-4a8b-ac33-29a71761efd4","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.606492Z","iopub.execute_input":"2025-05-03T18:18:57.606771Z","iopub.status.idle":"2025-05-03T18:18:57.622532Z","shell.execute_reply.started":"2025-05-03T18:18:57.606753Z","shell.execute_reply":"2025-05-03T18:18:57.621978Z"}},"outputs":[{"name":"stdout","text":"[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"\n\n*   We can decode the integers back into text\n\n","metadata":{"id":"cYXJhT8VsMEe"}},{"cell_type":"code","source":"tokenizer.decode(ids)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Ohlb0HApsKBi","outputId":"55a523a7-7ed2-4dc7-e565-f34384ef7672","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.623294Z","iopub.execute_input":"2025-05-03T18:18:57.623536Z","iopub.status.idle":"2025-05-03T18:18:57.638023Z","shell.execute_reply.started":"2025-05-03T18:18:57.623512Z","shell.execute_reply":"2025-05-03T18:18:57.637361Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"tokenizer.decode(tokenizer.encode(text))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"pvuZGhYmsP_m","outputId":"9c3064a1-fe52-4671-a3fc-19b3090c46c9","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.638677Z","iopub.execute_input":"2025-05-03T18:18:57.638970Z","iopub.status.idle":"2025-05-03T18:18:57.653671Z","shell.execute_reply.started":"2025-05-03T18:18:57.638947Z","shell.execute_reply":"2025-05-03T18:18:57.653142Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"**2.4 Adding special context tokens**\n\n\n\n*   It's useful to add some \"special\" tokens for unknown words and to denote the end of a text\n*   Some tokenizers use special tokens to help the LLM with additional context\n*   Some of these special tokens are\n\n[BOS] (beginning of sequence) marks the beginning of text\n[EOS] (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n[PAD] (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n*   [UNK] to represent words that are not included in the vocabulary\n\n*   We use the <|endoftext|> tokens between two independent sources of text:\n*   Let's see what happens if we tokenize the following text:\n\n\n\n\n\n\n\n\n","metadata":{"id":"VHKrql1GsVRZ"}},{"cell_type":"code","source":"tokenizer = SimpleTokenizerV1(vocab)\n\ntext = \"Hello, do you like tea. Is this-- a test?\"\n\n# tokenizer.encode(text)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"QDDHyxpJeUeA","outputId":"e8e8ebc8-527b-419c-8061-f9699ace99d9","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.654412Z","iopub.execute_input":"2025-05-03T18:18:57.654607Z","iopub.status.idle":"2025-05-03T18:18:57.668768Z","shell.execute_reply.started":"2025-05-03T18:18:57.654584Z","shell.execute_reply":"2025-05-03T18:18:57.668142Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"\n\n*   The above produces an error because the word \"Hello\" is not contained in the vocabulary\n*   To deal with such cases, we can add special tokens like \"<|unk|>\" to the vocabulary to represent unknown words\n*   Since we are already extending the vocabulary, let's add another token called \"<|endoftext|>\" which is used in GPT-2 training to denote the end of a text (and it's also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)\n\n\n\n","metadata":{"id":"0vidFcJUuau8"}},{"cell_type":"code","source":"all_tokens = sorted(list(set(preprocessed)))\nall_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n\nvocab = {token:integer for integer,token in enumerate(all_tokens)}","metadata":{"id":"tenLUqI3uYkF","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.669545Z","iopub.execute_input":"2025-05-03T18:18:57.670012Z","iopub.status.idle":"2025-05-03T18:18:57.684768Z","shell.execute_reply.started":"2025-05-03T18:18:57.669995Z","shell.execute_reply":"2025-05-03T18:18:57.684140Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"len(vocab.items())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ik3OVSzPuwjc","outputId":"07c2145c-0165-46e5-a902-863da5bd5867","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.685493Z","iopub.execute_input":"2025-05-03T18:18:57.685721Z","iopub.status.idle":"2025-05-03T18:18:57.701563Z","shell.execute_reply.started":"2025-05-03T18:18:57.685701Z","shell.execute_reply":"2025-05-03T18:18:57.700968Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"1132"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"for i, item in enumerate(list(vocab.items())[-5:]):\n    print(item)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsuPMlmzuzb0","outputId":"f132c43e-2175-4975-bdeb-4a2893e8f61e","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.702173Z","iopub.execute_input":"2025-05-03T18:18:57.702341Z","iopub.status.idle":"2025-05-03T18:18:57.715389Z","shell.execute_reply.started":"2025-05-03T18:18:57.702327Z","shell.execute_reply":"2025-05-03T18:18:57.714694Z"}},"outputs":[{"name":"stdout","text":"('younger', 1127)\n('your', 1128)\n('yourself', 1129)\n('<|endoftext|>', 1130)\n('<|unk|>', 1131)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"\n\n*   We also need to adjust the tokenizer accordingly so that it knows when and how to use the new <unk> token\n\n","metadata":{"id":"aXsmCbGNu2I_"}},{"cell_type":"code","source":"class SimpleTokenizerV2:\n    def __init__(self, vocab):\n        self.str_to_int = vocab\n        self.int_to_str = { i:s for s,i in vocab.items()}\n\n    def encode(self, text):\n        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n        preprocessed = [\n            item if item in self.str_to_int\n            else \"<|unk|>\" for item in preprocessed\n        ]\n\n        ids = [self.str_to_int[s] for s in preprocessed]\n        return ids\n\n    def decode(self, ids):\n        text = \" \".join([self.int_to_str[i] for i in ids])\n        # Replace spaces before the specified punctuations\n        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n        return text","metadata":{"id":"zoiIB2Scu1fv","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.716192Z","iopub.execute_input":"2025-05-03T18:18:57.716403Z","iopub.status.idle":"2025-05-03T18:18:57.731807Z","shell.execute_reply.started":"2025-05-03T18:18:57.716388Z","shell.execute_reply":"2025-05-03T18:18:57.731063Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"\n\n*   Let's try to tokenize text with the modified tokenizer:\n\n","metadata":{"id":"rZx96lp4u-M-"}},{"cell_type":"code","source":"tokenizer = SimpleTokenizerV2(vocab)\n\ntext1 = \"Hello, do you like tea?\"\ntext2 = \"In the sunlit terraces of the palace.\"\n\ntext = \" <|endoftext|> \".join((text1, text2))\n\nprint(text)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlAFTpggu8t_","outputId":"9ea3c0ff-b82d-44dc-a2c4-a56d7776cc29","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.732501Z","iopub.execute_input":"2025-05-03T18:18:57.732737Z","iopub.status.idle":"2025-05-03T18:18:57.751408Z","shell.execute_reply.started":"2025-05-03T18:18:57.732721Z","shell.execute_reply":"2025-05-03T18:18:57.750616Z"}},"outputs":[{"name":"stdout","text":"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"tokenizer.encode(text)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYyqfF9WvCyL","outputId":"2465174b-5535-4f5c-850e-1b14b26341ee","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.752343Z","iopub.execute_input":"2025-05-03T18:18:57.752602Z","iopub.status.idle":"2025-05-03T18:18:57.767029Z","shell.execute_reply.started":"2025-05-03T18:18:57.752580Z","shell.execute_reply":"2025-05-03T18:18:57.766355Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"tokenizer.decode(tokenizer.encode(text))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"6_41jZegvEVE","outputId":"81b1b364-7f7c-47b8-e0c6-a413d24310ed","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.767877Z","iopub.execute_input":"2025-05-03T18:18:57.768186Z","iopub.status.idle":"2025-05-03T18:18:57.782124Z","shell.execute_reply.started":"2025-05-03T18:18:57.768164Z","shell.execute_reply":"2025-05-03T18:18:57.781547Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"**2.5 BytePair encoding**\n\n\n\n\n*   GPT-2 used BytePair encoding (BPE) as its tokenizer\n*   it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n*   For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n*   The original BPE tokenizer can be found here: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n\n\n\n\n\n\n","metadata":{"id":"O9v7IH6ZvJ2K"}},{"cell_type":"code","source":"# pip install tiktoken\n\nimport importlib\nimport tiktoken\n\nprint(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_zm34eDvIbT","outputId":"3f9fe4aa-03a7-4958-886b-c8622e3331e3","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.782827Z","iopub.execute_input":"2025-05-03T18:18:57.783013Z","iopub.status.idle":"2025-05-03T18:18:57.831943Z","shell.execute_reply.started":"2025-05-03T18:18:57.782998Z","shell.execute_reply":"2025-05-03T18:18:57.831431Z"}},"outputs":[{"name":"stdout","text":"tiktoken version: 0.9.0\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"tokenizer = tiktoken.get_encoding(\"gpt2\")","metadata":{"id":"vUECcKOqvoMx","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:57.832690Z","iopub.execute_input":"2025-05-03T18:18:57.832933Z","iopub.status.idle":"2025-05-03T18:18:59.403427Z","shell.execute_reply.started":"2025-05-03T18:18:57.832910Z","shell.execute_reply":"2025-05-03T18:18:59.402839Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"text = (\n    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n     \"of someunknownPlace.\"\n)\n\nintegers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n\nprint(integers)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDorpPu6vqG6","outputId":"c68a7939-25b6-4c83-eddf-dc77756bc499","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:59.404161Z","iopub.execute_input":"2025-05-03T18:18:59.404404Z","iopub.status.idle":"2025-05-03T18:18:59.409236Z","shell.execute_reply.started":"2025-05-03T18:18:59.404382Z","shell.execute_reply":"2025-05-03T18:18:59.408600Z"}},"outputs":[{"name":"stdout","text":"[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"strings = tokenizer.decode(integers)\n\nprint(strings)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UJn7AWpvvmw","outputId":"a4f7035a-a009-4be0-ae7b-e298a8615257","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:18:59.410146Z","iopub.execute_input":"2025-05-03T18:18:59.410368Z","iopub.status.idle":"2025-05-03T18:19:02.731697Z","shell.execute_reply.started":"2025-05-03T18:18:59.410348Z","shell.execute_reply":"2025-05-03T18:19:02.731026Z"}},"outputs":[{"name":"stdout","text":"Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"**2.6 Data sampling with a sliding window**\n\n\n\n\n*   We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:\n\n","metadata":{"id":"Tucpk2dkvz17"}},{"cell_type":"code","source":"with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\n\nenc_text = tokenizer.encode(raw_text)\nprint(len(enc_text))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NjOP5eSvxoW","outputId":"95416b37-0753-4fd4-97a3-9cdeae0689f6","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:02.732377Z","iopub.execute_input":"2025-05-03T18:19:02.732571Z","iopub.status.idle":"2025-05-03T18:19:02.748676Z","shell.execute_reply.started":"2025-05-03T18:19:02.732555Z","shell.execute_reply":"2025-05-03T18:19:02.748156Z"}},"outputs":[{"name":"stdout","text":"5145\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"\n\n*   For each text chunk, we want the inputs and targets\n*   Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right\n\n","metadata":{"id":"0_4SVrQBv8ey"}},{"cell_type":"code","source":"enc_sample = enc_text[50:]","metadata":{"id":"s9Ly07c8v77G","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:02.749431Z","iopub.execute_input":"2025-05-03T18:19:02.749706Z","iopub.status.idle":"2025-05-03T18:19:02.759836Z","shell.execute_reply.started":"2025-05-03T18:19:02.749690Z","shell.execute_reply":"2025-05-03T18:19:02.759095Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"context_size = 4\n\nx = enc_sample[:context_size]\ny = enc_sample[1:context_size+1]\n\nprint(f\"x: {x}\")\nprint(f\"y:      {y}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CcbT2SrZwEH3","outputId":"0ecec84f-7f42-4f1d-8c72-5b5614405859","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:02.760574Z","iopub.execute_input":"2025-05-03T18:19:02.760760Z","iopub.status.idle":"2025-05-03T18:19:02.774053Z","shell.execute_reply.started":"2025-05-03T18:19:02.760737Z","shell.execute_reply":"2025-05-03T18:19:02.773554Z"}},"outputs":[{"name":"stdout","text":"x: [290, 4920, 2241, 287]\ny:      [4920, 2241, 287, 257]\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"One by one, the prediction would look like as follows:","metadata":{"id":"6SvC4465wIVP"}},{"cell_type":"code","source":"for i in range(1, context_size+1):\n    context = enc_sample[:i]\n    desired = enc_sample[i]\n\n    print(context, \"---->\", desired)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WSnrT7VawF5W","outputId":"ed71e180-c499-4fde-c23b-c79b7db91cfc","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:02.774858Z","iopub.execute_input":"2025-05-03T18:19:02.775511Z","iopub.status.idle":"2025-05-03T18:19:02.790667Z","shell.execute_reply.started":"2025-05-03T18:19:02.775487Z","shell.execute_reply":"2025-05-03T18:19:02.790083Z"}},"outputs":[{"name":"stdout","text":"[290] ----> 4920\n[290, 4920] ----> 2241\n[290, 4920, 2241] ----> 287\n[290, 4920, 2241, 287] ----> 257\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"for i in range(1, context_size+1):\n    context = enc_sample[:i]\n    desired = enc_sample[i]\n\n    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtokYp1AwLLH","outputId":"da6dea97-f7f1-45e8-8584-59361b4a6fbf","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:02.791345Z","iopub.execute_input":"2025-05-03T18:19:02.791577Z","iopub.status.idle":"2025-05-03T18:19:02.805465Z","shell.execute_reply.started":"2025-05-03T18:19:02.791560Z","shell.execute_reply":"2025-05-03T18:19:02.804769Z"}},"outputs":[{"name":"stdout","text":" and ---->  established\n and established ---->  himself\n and established himself ---->  in\n and established himself in ---->  a\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"\n\n*   We will take care of the next-word prediction in a later chapter after we covered the attention mechanism\n*   For now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one\n*   Install and import PyTorch (see Appendix A for installation tips)\n\n\n\n\n","metadata":{"id":"vBd6JHigwQuu"}},{"cell_type":"code","source":"import torch\nprint(\"PyTorch version:\", torch.__version__)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSNDkb_zwO0j","outputId":"223bc7e3-585c-49ad-a3ce-ba0c68843447","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:02.806150Z","iopub.execute_input":"2025-05-03T18:19:02.806403Z","iopub.status.idle":"2025-05-03T18:19:06.029034Z","shell.execute_reply.started":"2025-05-03T18:19:02.806381Z","shell.execute_reply":"2025-05-03T18:19:06.028270Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.5.1+cu124\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"\n\n*   We use a sliding window approach, changing the position by +1:\n*   Create dataset and dataloader that extract chunks from the input text dataset\n\n","metadata":{"id":"S3peTNh4wc36"}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        # Tokenize the entire text\n        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]","metadata":{"id":"hqixvXhZwZlH","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.029823Z","iopub.execute_input":"2025-05-03T18:19:06.030235Z","iopub.status.idle":"2025-05-03T18:19:06.035338Z","shell.execute_reply.started":"2025-05-03T18:19:06.030215Z","shell.execute_reply":"2025-05-03T18:19:06.034640Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def create_dataloader_v1(txt, batch_size=4, max_length=256,\n                         stride=128, shuffle=True, drop_last=True,\n                         num_workers=0):\n\n    # Initialize the tokenizer\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    # Create dataset\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n\n    # Create dataloader\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        drop_last=drop_last,\n        num_workers=num_workers\n    )\n\n    return dataloader","metadata":{"id":"2mOf6XLDwkK5","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.036182Z","iopub.execute_input":"2025-05-03T18:19:06.036653Z","iopub.status.idle":"2025-05-03T18:19:06.053960Z","shell.execute_reply.started":"2025-05-03T18:19:06.036625Z","shell.execute_reply":"2025-05-03T18:19:06.053222Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"\n\n*   Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4:\n\n","metadata":{"id":"DqpCxxWHwtpI"}},{"cell_type":"code","source":"with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()","metadata":{"id":"qL5LWOELwrPE","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.054799Z","iopub.execute_input":"2025-05-03T18:19:06.055055Z","iopub.status.idle":"2025-05-03T18:19:06.068759Z","shell.execute_reply.started":"2025-05-03T18:19:06.055032Z","shell.execute_reply":"2025-05-03T18:19:06.068049Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"dataloader = create_dataloader_v1(\n    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n)\n\ndata_iter = iter(dataloader)\nfirst_batch = next(data_iter)\nprint(first_batch)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"48_PQaJJwxfK","outputId":"e3350423-6a53-4b0f-caae-e3cb517907ff","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.069532Z","iopub.execute_input":"2025-05-03T18:19:06.069823Z","iopub.status.idle":"2025-05-03T18:19:06.191333Z","shell.execute_reply.started":"2025-05-03T18:19:06.069797Z","shell.execute_reply":"2025-05-03T18:19:06.190711Z"}},"outputs":[{"name":"stdout","text":"[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"second_batch = next(data_iter)\nprint(second_batch)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x1XZh6ogwziZ","outputId":"21bc9284-bbd4-449e-b6ba-613f34ec07e9","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.192029Z","iopub.execute_input":"2025-05-03T18:19:06.192271Z","iopub.status.idle":"2025-05-03T18:19:06.196789Z","shell.execute_reply.started":"2025-05-03T18:19:06.192253Z","shell.execute_reply":"2025-05-03T18:19:06.196287Z"}},"outputs":[{"name":"stdout","text":"[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"\n\n*   An example using stride equal to the context length (here: 4) as shown below:\n\n","metadata":{"id":"8xKQdVHlxeDw"}},{"cell_type":"markdown","source":"\n\n*   We can also create batched outputs\n*   Note that we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting\n\n","metadata":{"id":"OqXkmroW3va3"}},{"cell_type":"code","source":"dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n\ndata_iter = iter(dataloader)\ninputs, targets = next(data_iter)\nprint(\"Inputs:\\n\", inputs)\nprint(\"\\nTargets:\\n\", targets)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mrm2m0v9w1mS","outputId":"820ace04-c4ee-4be6-c9c8-a8244030e92c","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.204239Z","iopub.execute_input":"2025-05-03T18:19:06.204688Z","iopub.status.idle":"2025-05-03T18:19:06.235204Z","shell.execute_reply.started":"2025-05-03T18:19:06.204671Z","shell.execute_reply":"2025-05-03T18:19:06.234478Z"}},"outputs":[{"name":"stdout","text":"Inputs:\n tensor([[   40,   367,  2885,  1464],\n        [ 1807,  3619,   402,   271],\n        [10899,  2138,   257,  7026],\n        [15632,   438,  2016,   257],\n        [  922,  5891,  1576,   438],\n        [  568,   340,   373,   645],\n        [ 1049,  5975,   284,   502],\n        [  284,  3285,   326,    11]])\n\nTargets:\n tensor([[  367,  2885,  1464,  1807],\n        [ 3619,   402,   271, 10899],\n        [ 2138,   257,  7026, 15632],\n        [  438,  2016,   257,   922],\n        [ 5891,  1576,   438,   568],\n        [  340,   373,   645,  1049],\n        [ 5975,   284,   502,   284],\n        [ 3285,   326,    11,   287]])\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"**2.7 Creating token embeddings**\n\n\n\n\n*   The data is already almost ready for an LLM\n*   Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):\n\n\n\n\n\n\n\n","metadata":{"id":"ulymI3nx37LK"}},{"cell_type":"code","source":"input_ids = torch.tensor([2, 3, 5, 1])","metadata":{"id":"PiETq-et3p6v","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.236009Z","iopub.execute_input":"2025-05-03T18:19:06.236305Z","iopub.status.idle":"2025-05-03T18:19:06.239615Z","shell.execute_reply.started":"2025-05-03T18:19:06.236280Z","shell.execute_reply":"2025-05-03T18:19:06.239067Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"\n\n*   For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:\n\n","metadata":{"id":"_0FytzBy4ZW6"}},{"cell_type":"code","source":"vocab_size = 6\noutput_dim = 3\n\ntorch.manual_seed(123)\nembedding_layer = torch.nn.Embedding(vocab_size, output_dim)","metadata":{"id":"od0ptbpw4ZAb","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.240310Z","iopub.execute_input":"2025-05-03T18:19:06.240503Z","iopub.status.idle":"2025-05-03T18:19:06.257942Z","shell.execute_reply.started":"2025-05-03T18:19:06.240479Z","shell.execute_reply":"2025-05-03T18:19:06.257413Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"\n\n*   This would result in a 6x3 weight matrix:\n\n","metadata":{"id":"bSYMZ5BR4hRN"}},{"cell_type":"code","source":"print(embedding_layer.weight)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZ17F2G64gne","outputId":"be26aacb-b22f-4175-c0e4-d3f40b915ce8","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.258688Z","iopub.execute_input":"2025-05-03T18:19:06.258907Z","iopub.status.idle":"2025-05-03T18:19:06.299449Z","shell.execute_reply.started":"2025-05-03T18:19:06.258891Z","shell.execute_reply":"2025-05-03T18:19:06.298637Z"}},"outputs":[{"name":"stdout","text":"Parameter containing:\ntensor([[ 0.3374, -0.1778, -0.1690],\n        [ 0.9178,  1.5810,  1.3010],\n        [ 1.2753, -0.2010, -0.1606],\n        [-0.4015,  0.9666, -1.1481],\n        [-1.1589,  0.3255, -0.6315],\n        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"\n\n*   For those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully-connected layer, which is described in the supplementary code in ./embedding_vs_matmul\n*   To convert a token with id 3 into a 3-dimensional vector, we do the following:\n\n\n\n","metadata":{"id":"Wg9Na5oe4oop"}},{"cell_type":"code","source":"print(embedding_layer(torch.tensor([3])))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lYCAMObl4oAL","outputId":"43f0e831-3a2d-49ce-9131-5bf8f59d98dc","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.300199Z","iopub.execute_input":"2025-05-03T18:19:06.300492Z","iopub.status.idle":"2025-05-03T18:19:06.311822Z","shell.execute_reply.started":"2025-05-03T18:19:06.300469Z","shell.execute_reply":"2025-05-03T18:19:06.311231Z"}},"outputs":[{"name":"stdout","text":"tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"\n\n*   Note that the above is the 4th row in the embedding_layer weight matrix\n*   To embed all four input_ids values above, we do\n\n","metadata":{"id":"Kpf-YzWv40Xc"}},{"cell_type":"code","source":"print(embedding_layer(input_ids))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jt2a5pQ34z8m","outputId":"94d4a775-e702-4442-b303-9db81a2dd8d1","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.312617Z","iopub.execute_input":"2025-05-03T18:19:06.312867Z","iopub.status.idle":"2025-05-03T18:19:06.317989Z","shell.execute_reply.started":"2025-05-03T18:19:06.312850Z","shell.execute_reply":"2025-05-03T18:19:06.317343Z"}},"outputs":[{"name":"stdout","text":"tensor([[ 1.2753, -0.2010, -0.1606],\n        [-0.4015,  0.9666, -1.1481],\n        [-2.8400, -0.7849, -1.4096],\n        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"\n\n*   An embedding layer is essentially a look-up operation:\n*   You may be interested in the bonus content comparing embedding layers with regular linear layers: ../03_bonus_embedding-vs-matmul\n\n","metadata":{"id":"EeZZfvzr49u9"}},{"cell_type":"markdown","source":"**2.8 Encoding word positions**\n\n\n\n\n*   Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence:\n*   Suppose we want to encode the input tokens into a 256-dimensional vector representation:\n\n\n\n","metadata":{"id":"vA4Q83PI5Fvv"}},{"cell_type":"code","source":"vocab_size = 50257\noutput_dim = 256\n\ntoken_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)","metadata":{"id":"GvaRF32q47MU","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.318762Z","iopub.execute_input":"2025-05-03T18:19:06.319168Z","iopub.status.idle":"2025-05-03T18:19:06.449132Z","shell.execute_reply.started":"2025-05-03T18:19:06.319146Z","shell.execute_reply":"2025-05-03T18:19:06.448467Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"\n\n*   If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\n*   If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:\n\n","metadata":{"id":"jUk4FKt25UbF"}},{"cell_type":"code","source":"max_length = 4\ndataloader = create_dataloader_v1(\n    raw_text, batch_size=8, max_length=max_length,\n    stride=max_length, shuffle=False\n)\ndata_iter = iter(dataloader)\ninputs, targets = next(data_iter)","metadata":{"id":"4ddQjUC15T8v","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.449842Z","iopub.execute_input":"2025-05-03T18:19:06.450072Z","iopub.status.idle":"2025-05-03T18:19:06.473468Z","shell.execute_reply.started":"2025-05-03T18:19:06.450049Z","shell.execute_reply":"2025-05-03T18:19:06.472925Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"print(\"Token IDs:\\n\", inputs)\nprint(\"\\nInputs shape:\\n\", inputs.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJ_5pq5M5c1O","outputId":"f0c46ab7-c1ee-4204-e5b5-1bcca864c321","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.474440Z","iopub.execute_input":"2025-05-03T18:19:06.474672Z","iopub.status.idle":"2025-05-03T18:19:06.479347Z","shell.execute_reply.started":"2025-05-03T18:19:06.474655Z","shell.execute_reply":"2025-05-03T18:19:06.478728Z"}},"outputs":[{"name":"stdout","text":"Token IDs:\n tensor([[   40,   367,  2885,  1464],\n        [ 1807,  3619,   402,   271],\n        [10899,  2138,   257,  7026],\n        [15632,   438,  2016,   257],\n        [  922,  5891,  1576,   438],\n        [  568,   340,   373,   645],\n        [ 1049,  5975,   284,   502],\n        [  284,  3285,   326,    11]])\n\nInputs shape:\n torch.Size([8, 4])\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"token_embeddings = token_embedding_layer(inputs)\nprint(token_embeddings.shape)\n\n# uncomment & execute the following line to see how the embeddings look like\n# print(token_embeddings)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ryPWaFL45egp","outputId":"f6a02d7c-6c65-4bd2-b28e-9f22757724cf","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.480141Z","iopub.execute_input":"2025-05-03T18:19:06.480412Z","iopub.status.idle":"2025-05-03T18:19:06.495863Z","shell.execute_reply.started":"2025-05-03T18:19:06.480388Z","shell.execute_reply":"2025-05-03T18:19:06.495152Z"}},"outputs":[{"name":"stdout","text":"torch.Size([8, 4, 256])\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"\n\n*   GPT-2 uses absolute position embeddings, so we just create another embedding layer:\n\n\n","metadata":{"id":"3jeaEJXV5hm8"}},{"cell_type":"code","source":"context_length = max_length\npos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n\n# uncomment & execute the following line to see how the embedding layer weights look like\n# print(pos_embedding_layer.weight)","metadata":{"id":"IRUYXyRH5hNn","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.496651Z","iopub.execute_input":"2025-05-03T18:19:06.497396Z","iopub.status.idle":"2025-05-03T18:19:06.509751Z","shell.execute_reply.started":"2025-05-03T18:19:06.497367Z","shell.execute_reply":"2025-05-03T18:19:06.508974Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"pos_embeddings = pos_embedding_layer(torch.arange(max_length))\nprint(pos_embeddings.shape)\n\n# uncomment & execute the following line to see how the embeddings look like\n# print(pos_embeddings)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0SfdfjU5m0c","outputId":"049599f6-0ca5-4ad3-858e-5fe96ef786d0","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.510454Z","iopub.execute_input":"2025-05-03T18:19:06.510646Z","iopub.status.idle":"2025-05-03T18:19:06.526284Z","shell.execute_reply.started":"2025-05-03T18:19:06.510632Z","shell.execute_reply":"2025-05-03T18:19:06.525520Z"}},"outputs":[{"name":"stdout","text":"torch.Size([4, 256])\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"\n\n*   To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:\n\n","metadata":{"id":"X5AiOjh05qhG"}},{"cell_type":"code","source":"input_embeddings = token_embeddings + pos_embeddings\nprint(input_embeddings.shape)\n\n# uncomment & execute the following line to see how the embeddings look like\n# print(input_embeddings)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z6Sjih5z5oqQ","outputId":"4afb3ae9-3478-451d-93ad-ff83f56067bb","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.527126Z","iopub.execute_input":"2025-05-03T18:19:06.527398Z","iopub.status.idle":"2025-05-03T18:19:06.540288Z","shell.execute_reply.started":"2025-05-03T18:19:06.527371Z","shell.execute_reply":"2025-05-03T18:19:06.539584Z"}},"outputs":[{"name":"stdout","text":"torch.Size([8, 4, 256])\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"\n\n*   In the initial phase of the input processing workflow, the input text is segmented into separate tokens\n*   Following this segmentation, these tokens are transformed into token IDs based on a predefined vocabulary:\n\n","metadata":{"id":"-MgJHbmO5usV"}},{"cell_type":"markdown","source":"**The Main Data Loading Pipeline Summarized**\n\n\n\n*   This notebook contains the main takeaway, the data loading pipeline without the intermediate steps.\n*   Packages that are being used in this notebook:\n\n","metadata":{"id":"cg916dev6lvn"}},{"cell_type":"code","source":"# NBVAL_SKIP\nfrom importlib.metadata import version\n\nprint(\"torch version:\", version(\"torch\"))\nprint(\"tiktoken version:\", version(\"tiktoken\"))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVpmm_nf6tRr","outputId":"7e0cb7fb-108a-4c1a-968b-020f87f1f6ea","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.541020Z","iopub.execute_input":"2025-05-03T18:19:06.541224Z","iopub.status.idle":"2025-05-03T18:19:06.555001Z","shell.execute_reply.started":"2025-05-03T18:19:06.541209Z","shell.execute_reply":"2025-05-03T18:19:06.554398Z"}},"outputs":[{"name":"stdout","text":"torch version: 2.5.1+cu124\ntiktoken version: 0.9.0\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"import tiktoken\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        # Tokenize the entire text\n        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n\n\ndef create_dataloader_v1(txt, batch_size, max_length, stride,\n                         shuffle=True, drop_last=True, num_workers=0):\n    # Initialize the tokenizer\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    # Create dataset\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n\n    # Create dataloader\n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n\n    return dataloader\n\n\nwith open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\n\nvocab_size = 50257\noutput_dim = 256\ncontext_length = 1024\n\n\ntoken_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\npos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n\nbatch_size = 8\nmax_length = 4\ndataloader = create_dataloader_v1(\n    raw_text,\n    batch_size=batch_size,\n    max_length=max_length,\n    stride=max_length\n)","metadata":{"id":"sC61WxWc6y_e","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.555769Z","iopub.execute_input":"2025-05-03T18:19:06.556001Z","iopub.status.idle":"2025-05-03T18:19:06.707431Z","shell.execute_reply.started":"2025-05-03T18:19:06.555984Z","shell.execute_reply":"2025-05-03T18:19:06.706647Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"for batch in dataloader:\n    x, y = batch\n\n    token_embeddings = token_embedding_layer(x)\n    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n\n    input_embeddings = token_embeddings + pos_embeddings\n\n    break","metadata":{"id":"NZJ2ggNk7QVt","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.708284Z","iopub.execute_input":"2025-05-03T18:19:06.708524Z","iopub.status.idle":"2025-05-03T18:19:06.721678Z","shell.execute_reply.started":"2025-05-03T18:19:06.708501Z","shell.execute_reply":"2025-05-03T18:19:06.721051Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"print(input_embeddings.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X4fXWTAh7SFa","outputId":"b0ae64a8-45f0-4148-cf91-6b2d7a67c110","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.722563Z","iopub.execute_input":"2025-05-03T18:19:06.723200Z","iopub.status.idle":"2025-05-03T18:19:06.729545Z","shell.execute_reply.started":"2025-05-03T18:19:06.723180Z","shell.execute_reply":"2025-05-03T18:19:06.728907Z"}},"outputs":[{"name":"stdout","text":"torch.Size([8, 4, 256])\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"\n\n---\n\n\n\n---\n\n","metadata":{"id":"WgCtc06IDMn7"}},{"cell_type":"markdown","source":"**Chapter 3: Coding Attention Mechanisms**","metadata":{"id":"HZXAHqEA7ref"}},{"cell_type":"code","source":"from importlib.metadata import version\n\nprint(\"torch version:\", version(\"torch\"))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-SH_taKj7tFa","outputId":"517e82af-7cdc-40d3-e3e7-87d24f8187b8","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.730239Z","iopub.execute_input":"2025-05-03T18:19:06.730467Z","iopub.status.idle":"2025-05-03T18:19:06.750995Z","shell.execute_reply.started":"2025-05-03T18:19:06.730445Z","shell.execute_reply":"2025-05-03T18:19:06.750237Z"}},"outputs":[{"name":"stdout","text":"torch version: 2.5.1+cu124\n","output_type":"stream"}],"execution_count":58},{"cell_type":"markdown","source":"\n\n*   This chapter covers attention mechanisms, the engine of LLMs:\n\n","metadata":{"id":"4HqMFf-370Al"}},{"cell_type":"markdown","source":"**3.1 The problem with modeling long sequences**\n\n\n\n\n*   No code in this section\n*   Translating a text word by word isn't feasible due to the differences in grammatical structures between the source and target languages:\n\n\n","metadata":{"id":"yg6sITsM74fB"}},{"cell_type":"markdown","source":"**3.2 Capturing data dependencies with attention mechanisms**\n\n\n\n*   No code in this section\n*   Self-attention in transformers is a technique designed to enhance input representations by enabling each position in a sequence to engage with and determine the relevance of every other position within the same sequence\n\n\n\n","metadata":{"id":"X-drovxY8L-N"}},{"cell_type":"markdown","source":"**3.3 Attending to different parts of the input with self-attention**","metadata":{"id":"i6inmIjC88yS"}},{"cell_type":"markdown","source":"**3.3.1 A simple self-attention mechanism without trainable weights**\n\n\n\n\n*   The next section, section 3.3.2, will extend this simple attention mechanism to implement the real self-attention mechanism\n*   Suppose we are given an input sequence x^1\n to x^T\nThe input is a text (for example, a sentence like \"Your journey starts with one step\") that has already been converted into token embeddings as described in chapter 2\nFor instance x^1,\n is a d-dimensional vector representing the word \"Your\", and so forth\n*   Goal: compute context vectors z^i\n for each input sequence element x^i\n in x^1\n to x^T\n (where\n and\n have the same dimension)\nA context vector z^i\n is a weighted sum over the inputs\n to\n\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"cSHrsQ798-lM"}},{"cell_type":"code","source":"import torch\n\ninputs = torch.tensor(\n  [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)","metadata":{"id":"d5SJbRq17yE-","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.751864Z","iopub.execute_input":"2025-05-03T18:19:06.752422Z","iopub.status.idle":"2025-05-03T18:19:06.766494Z","shell.execute_reply.started":"2025-05-03T18:19:06.752403Z","shell.execute_reply":"2025-05-03T18:19:06.765855Z"}},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"\n\n*   (In this book, we follow the common machine learning and deep learning convention where training examples are represented as rows and feature values as columns; in the case of the tensor shown above, each row represents a word, and each column represents an embedding dimension)\n*   The primary objective of this section is to demonstrate how the context vector z^2\n is calculated using the second input sequence x^2,\n, as a query\n*   The first step is to compute the unnormalized attention scores by computing the dot product between the query x^2 and all other input tokens:\n\n\n\n\n\n","metadata":{"id":"OOx41lp5_Kzg"}},{"cell_type":"code","source":"query = inputs[1]  # 2nd input token is the query\n\nattn_scores_2 = torch.empty(inputs.shape[0])\nfor i, x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n\nprint(attn_scores_2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEaSakrY732J","outputId":"f6bca9a7-ef34-4df4-bc78-4eee6cde0ded","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.767289Z","iopub.execute_input":"2025-05-03T18:19:06.767523Z","iopub.status.idle":"2025-05-03T18:19:06.790523Z","shell.execute_reply.started":"2025-05-03T18:19:06.767506Z","shell.execute_reply":"2025-05-03T18:19:06.789762Z"}},"outputs":[{"name":"stdout","text":"tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"\n\n*   Side note: a dot product is essentially a shorthand for multiplying two vectors elements-wise and summing the resulting products:\n\n","metadata":{"id":"-7kI9Wbr_1NB"}},{"cell_type":"code","source":"res = 0.\n\nfor idx, element in enumerate(inputs[0]):\n    res += inputs[0][idx] * query[idx]\n\nprint(res)\nprint(torch.dot(inputs[0], query))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GInSam5D_zdI","outputId":"ed28ee3d-4004-4a80-cfe4-0969a2cbe9f4","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.791238Z","iopub.execute_input":"2025-05-03T18:19:06.791488Z","iopub.status.idle":"2025-05-03T18:19:06.797491Z","shell.execute_reply.started":"2025-05-03T18:19:06.791470Z","shell.execute_reply":"2025-05-03T18:19:06.796917Z"}},"outputs":[{"name":"stdout","text":"tensor(0.9544)\ntensor(0.9544)\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"\n\n*   Step 2: normalize the unnormalized attention scores (\"omegas\", w) so that they sum up to 1\n*   Here is a simple way to normalize the unnormalized attention scores to sum up to 1 (a convention, useful for interpretation, and important for training stability):\n\n","metadata":{"id":"OJoJHo2W_9_V"}},{"cell_type":"code","source":"attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n\nprint(\"Attention weights:\", attn_weights_2_tmp)\nprint(\"Sum:\", attn_weights_2_tmp.sum())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65Ae5WHSAEI8","outputId":"649436f6-806c-4127-e470-418a678db467","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.798215Z","iopub.execute_input":"2025-05-03T18:19:06.798476Z","iopub.status.idle":"2025-05-03T18:19:06.815407Z","shell.execute_reply.started":"2025-05-03T18:19:06.798460Z","shell.execute_reply":"2025-05-03T18:19:06.814818Z"}},"outputs":[{"name":"stdout","text":"Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\nSum: tensor(1.0000)\n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"\n\n\n*   Here's a naive implementation of a softmax function for scaling, which also normalizes the vector elements such that they sum up to 1:\n\n","metadata":{"id":"LZCFCN8UAJsL"}},{"cell_type":"code","source":"def softmax_naive(x):\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\n\nattn_weights_2_naive = softmax_naive(attn_scores_2)\n\nprint(\"Attention weights:\", attn_weights_2_naive)\nprint(\"Sum:\", attn_weights_2_naive.sum())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Of4SzlvGAJBF","outputId":"bbcdb8c1-619e-4bee-9ae7-f43a06588f91","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.816164Z","iopub.execute_input":"2025-05-03T18:19:06.816424Z","iopub.status.idle":"2025-05-03T18:19:06.838943Z","shell.execute_reply.started":"2025-05-03T18:19:06.816406Z","shell.execute_reply":"2025-05-03T18:19:06.838321Z"}},"outputs":[{"name":"stdout","text":"Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum: tensor(1.)\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"\n\n*   The naive implementation above can suffer from numerical instability issues for large or small input values due to overflow and underflow issues\n*   Hence, in practice, it's recommended to use the PyTorch implementation of softmax instead, which has been highly optimized for performance:\n","metadata":{"id":"o7P_nGPcAS47"}},{"cell_type":"code","source":"attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n\nprint(\"Attention weights:\", attn_weights_2)\nprint(\"Sum:\", attn_weights_2.sum())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xuyBWYmFAQ9O","outputId":"e1d29676-29b9-4458-f169-f7f5bb4e51e9","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.839717Z","iopub.execute_input":"2025-05-03T18:19:06.839906Z","iopub.status.idle":"2025-05-03T18:19:06.849623Z","shell.execute_reply.started":"2025-05-03T18:19:06.839890Z","shell.execute_reply":"2025-05-03T18:19:06.848957Z"}},"outputs":[{"name":"stdout","text":"Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum: tensor(1.)\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"\n\n*   Step 3: compute the context vector z^2\n by multiplying the embedded input tokens, x^i\n with the attention weights and sum the resulting vectors:\n\n","metadata":{"id":"FghJVDbmAbSA"}},{"cell_type":"code","source":"query = inputs[1] # 2nd input token is the query\n\ncontext_vec_2 = torch.zeros(query.shape)\nfor i,x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i]*x_i\n\nprint(context_vec_2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_bJmpE-Aicr","outputId":"d0e1a82f-c932-4001-9668-e490346e2368","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.850476Z","iopub.execute_input":"2025-05-03T18:19:06.850934Z","iopub.status.idle":"2025-05-03T18:19:06.857182Z","shell.execute_reply.started":"2025-05-03T18:19:06.850909Z","shell.execute_reply":"2025-05-03T18:19:06.856575Z"}},"outputs":[{"name":"stdout","text":"tensor([0.4419, 0.6515, 0.5683])\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"**3.3.2 Computing attention weights for all input tokens**\n\nGeneralize to all input sequence tokens:\n\n*   Above, we computed the attention weights and context vector for input 2 (as illustrated in the highlighted row in the figure below)\n*   These attention weights are then utilized to generate the context vectors through a weighted summation of the inputs\n*   Apply previous step 1 to all pairwise elements to compute the unnormalized attention score matrix:\n\n\n\n\n\n","metadata":{"id":"1fB_qV6uAmPG"}},{"cell_type":"code","source":"attn_scores = torch.empty(6, 6)\n\nfor i, x_i in enumerate(inputs):\n    for j, x_j in enumerate(inputs):\n        attn_scores[i, j] = torch.dot(x_i, x_j)\n\nprint(attn_scores)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6O2Wa5bAkDK","outputId":"ee1c0ad0-80f0-42dc-8173-4be4951b5121","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.857939Z","iopub.execute_input":"2025-05-03T18:19:06.858463Z","iopub.status.idle":"2025-05-03T18:19:06.868907Z","shell.execute_reply.started":"2025-05-03T18:19:06.858438Z","shell.execute_reply":"2025-05-03T18:19:06.868326Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"\n\n*   We can achieve the same as above more efficiently via matrix multiplication:\n\n","metadata":{"id":"7yN4gK85BCMD"}},{"cell_type":"code","source":"attn_scores = inputs @ inputs.T\nprint(attn_scores)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w3EI2xTzBAbg","outputId":"520f33df-6457-4b84-e0c4-c7301d3d6928","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.869555Z","iopub.execute_input":"2025-05-03T18:19:06.869774Z","iopub.status.idle":"2025-05-03T18:19:06.892902Z","shell.execute_reply.started":"2025-05-03T18:19:06.869759Z","shell.execute_reply":"2025-05-03T18:19:06.892327Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"\n\n*   Similar to step 2 previously, we normalize each row so that the values in each row sum to 1:\n\n","metadata":{"id":"PeuKgl1LBKCh"}},{"cell_type":"code","source":"attn_weights = torch.softmax(attn_scores, dim=-1)\nprint(attn_weights)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CISvLJI_BDpy","outputId":"6e6aaccd-f05d-4f49-d986-b6a502aad4c1","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.893670Z","iopub.execute_input":"2025-05-03T18:19:06.893897Z","iopub.status.idle":"2025-05-03T18:19:06.898935Z","shell.execute_reply.started":"2025-05-03T18:19:06.893880Z","shell.execute_reply":"2025-05-03T18:19:06.898301Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"\n\n*   Quick verification that the values in each row indeed sum to 1:\n\n","metadata":{"id":"LcHQRQYPBQIc"}},{"cell_type":"code","source":"row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nprint(\"Row 2 sum:\", row_2_sum)\n\nprint(\"All row sums:\", attn_weights.sum(dim=-1))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJnWU3brBLXT","outputId":"4a84901b-c138-491f-fff0-ab26dec34dcb","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.899707Z","iopub.execute_input":"2025-05-03T18:19:06.899940Z","iopub.status.idle":"2025-05-03T18:19:06.913624Z","shell.execute_reply.started":"2025-05-03T18:19:06.899925Z","shell.execute_reply":"2025-05-03T18:19:06.912921Z"}},"outputs":[{"name":"stdout","text":"Row 2 sum: 1.0\nAll row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n","output_type":"stream"}],"execution_count":69},{"cell_type":"markdown","source":"\n\n*   Apply previous step 3 to compute all context vectors:\n\n","metadata":{"id":"PUy3CZ-1BV5y"}},{"cell_type":"code","source":"all_context_vecs = attn_weights @ inputs\nprint(all_context_vecs)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RpJXi6aTBRnC","outputId":"6f738135-a87e-4255-f606-90b67f6c5801","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.914455Z","iopub.execute_input":"2025-05-03T18:19:06.914727Z","iopub.status.idle":"2025-05-03T18:19:06.930825Z","shell.execute_reply.started":"2025-05-03T18:19:06.914703Z","shell.execute_reply":"2025-05-03T18:19:06.930214Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])\n","output_type":"stream"}],"execution_count":70},{"cell_type":"markdown","source":"\n\n*   As a sanity check, the previously computed context vector z^2 = [0.4419, 0.6515, 0.5683]can be found in the 2nd row in above:\n\n","metadata":{"id":"cZzDzTUEBhSM"}},{"cell_type":"code","source":"print(\"Previous 2nd context vector:\", context_vec_2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlXgPuxHBXgP","outputId":"7b6b42fc-aafc-470e-e936-e2523f62ba91","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:06.931690Z","iopub.execute_input":"2025-05-03T18:19:06.931938Z","iopub.status.idle":"2025-05-03T18:19:07.040130Z","shell.execute_reply.started":"2025-05-03T18:19:06.931916Z","shell.execute_reply":"2025-05-03T18:19:07.039526Z"}},"outputs":[{"name":"stdout","text":"Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n","output_type":"stream"}],"execution_count":71},{"cell_type":"markdown","source":"**3.4 Implementing self-attention with trainable weights**\n\n\n\n*   A conceptual framework illustrating how the self-attention mechanism developed in this section integrates into the overall narrative and structure of this book and chapter\n\n","metadata":{"id":"NawXx_iLB9D6"}},{"cell_type":"markdown","source":"**3.4.1 Computing the attention weights step by step**\n\n\n\n*   In this section, we are implementing the self-attention mechanism that is used in the original transformer architecture, the GPT models, and most other popular LLMs\n*   This self-attention mechanism is also called \"scaled dot-product attention\"\n*   The overall idea is similar to before:\nWe want to compute context vectors as weighted sums over the input vectors specific to a certain input element\nFor the above, we need attention weights\n*   As you will see, there are only slight differences compared to the basic attention mechanism introduced earlier:\n\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"1PQRBIxwCDGl"}},{"cell_type":"code","source":"x_2 = inputs[1] # second input element\nd_in = inputs.shape[1] # the input embedding size, d=3\nd_out = 2 # the output embedding size, d=2","metadata":{"id":"cas94v3-Bykt","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.040886Z","iopub.execute_input":"2025-05-03T18:19:07.041329Z","iopub.status.idle":"2025-05-03T18:19:07.054850Z","shell.execute_reply.started":"2025-05-03T18:19:07.041302Z","shell.execute_reply":"2025-05-03T18:19:07.054150Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"\n\n*   Below, we initialize the three weight matrices; note that we are setting requires_grad=False to reduce clutter in the outputs for illustration purposes, but if we were to use the weight matrices for model training, we would set requires_grad=True to update these matrices during model training\n\n","metadata":{"id":"A-PMEcnlCyAT"}},{"cell_type":"code","source":"torch.manual_seed(123)\n\nW_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)","metadata":{"id":"q3SPHsNECvzS","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.055591Z","iopub.execute_input":"2025-05-03T18:19:07.055851Z","iopub.status.idle":"2025-05-03T18:19:07.072159Z","shell.execute_reply.started":"2025-05-03T18:19:07.055833Z","shell.execute_reply":"2025-05-03T18:19:07.071517Z"}},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":"\n\n*   Next we compute the query, key, and value vectors:\n\n\n\n\n","metadata":{"id":"ghkaKjEFC2XA"}},{"cell_type":"code","source":"query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\nkey_2 = x_2 @ W_key\nvalue_2 = x_2 @ W_value\n\nprint(query_2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NZ0LTLuC1hx","outputId":"8369add4-6f4c-4f08-bcc0-95df35667c7e","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.072873Z","iopub.execute_input":"2025-05-03T18:19:07.073141Z","iopub.status.idle":"2025-05-03T18:19:07.090035Z","shell.execute_reply.started":"2025-05-03T18:19:07.073100Z","shell.execute_reply":"2025-05-03T18:19:07.089351Z"}},"outputs":[{"name":"stdout","text":"tensor([0.4306, 1.4551])\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"keys = inputs @ W_key\nvalues = inputs @ W_value\n\nprint(\"keys.shape:\", keys.shape)\nprint(\"values.shape:\", values.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rMI6pvPVC93y","outputId":"33c1fbdb-67b2-404d-cb62-5a4b4f4c793c","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.090952Z","iopub.execute_input":"2025-05-03T18:19:07.091334Z","iopub.status.idle":"2025-05-03T18:19:07.101655Z","shell.execute_reply.started":"2025-05-03T18:19:07.091306Z","shell.execute_reply":"2025-05-03T18:19:07.101025Z"}},"outputs":[{"name":"stdout","text":"keys.shape: torch.Size([6, 2])\nvalues.shape: torch.Size([6, 2])\n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"\n\n*   In the next step, step 2, we compute the unnormalized attention scores by computing the dot product between the query and each key vector:\n\n\n","metadata":{"id":"jIPWTYDjDSfM"}},{"cell_type":"code","source":"keys_2 = keys[1] # Python starts index at 0\nattn_score_22 = query_2.dot(keys_2)\nprint(attn_score_22)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1-s_kioDRsb","outputId":"1218f3ce-0c22-46ec-a4df-13e919d56eb9","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.102313Z","iopub.execute_input":"2025-05-03T18:19:07.102612Z","iopub.status.idle":"2025-05-03T18:19:07.116850Z","shell.execute_reply.started":"2025-05-03T18:19:07.102595Z","shell.execute_reply":"2025-05-03T18:19:07.116138Z"}},"outputs":[{"name":"stdout","text":"tensor(1.8524)\n","output_type":"stream"}],"execution_count":76},{"cell_type":"markdown","source":"\n\n*   Since we have 6 inputs, we have 6 attention scores for the given query vector:\n\n","metadata":{"id":"1b_NwAyDDZbv"}},{"cell_type":"code","source":"attn_scores_2 = query_2 @ keys.T # All attention scores for given query\nprint(attn_scores_2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-tAngt1DYt9","outputId":"6488b78d-c7a5-4295-b481-a752bf2d7476","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.117516Z","iopub.execute_input":"2025-05-03T18:19:07.117687Z","iopub.status.idle":"2025-05-03T18:19:07.131975Z","shell.execute_reply.started":"2025-05-03T18:19:07.117665Z","shell.execute_reply":"2025-05-03T18:19:07.131248Z"}},"outputs":[{"name":"stdout","text":"tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n","output_type":"stream"}],"execution_count":77},{"cell_type":"markdown","source":"\n\n*   Next, in step 3, we compute the attention weights (normalized attention scores that sum up to 1) using the softmax function we used earlier\n*   The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension, (i.e., d_k**0.5):\n\n","metadata":{"id":"-q7iutmNDgnI"}},{"cell_type":"code","source":"d_k = keys.shape[1]\nattn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\nprint(attn_weights_2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jxz4fLOIDe0r","outputId":"01698469-e65c-44cc-c909-9db2616dc0bd","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.132859Z","iopub.execute_input":"2025-05-03T18:19:07.133088Z","iopub.status.idle":"2025-05-03T18:19:07.146557Z","shell.execute_reply.started":"2025-05-03T18:19:07.133073Z","shell.execute_reply":"2025-05-03T18:19:07.145997Z"}},"outputs":[{"name":"stdout","text":"tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n","output_type":"stream"}],"execution_count":78},{"cell_type":"markdown","source":"\n\n*   In step 4, we now compute the context vector for input query vector 2:\n\n\n","metadata":{"id":"PsqQMtcaDpwF"}},{"cell_type":"code","source":"context_vec_2 = attn_weights_2 @ values\nprint(context_vec_2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SeZ9qspSDmma","outputId":"9e01baef-2812-4ed0-e22c-98c7c7aadd84","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.147242Z","iopub.execute_input":"2025-05-03T18:19:07.147790Z","iopub.status.idle":"2025-05-03T18:19:07.160892Z","shell.execute_reply.started":"2025-05-03T18:19:07.147772Z","shell.execute_reply":"2025-05-03T18:19:07.160329Z"}},"outputs":[{"name":"stdout","text":"tensor([0.3061, 0.8210])\n","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"**3.4.2 Implementing a compact SelfAttention class**\n\n\n\n*   Putting it all together, we can implement the self-attention mechanism as follows:\n\n","metadata":{"id":"3jJDcvaFEa9P"}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass SelfAttention_v1(nn.Module):\n\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n\n    def forward(self, x):\n        keys = x @ self.W_key\n        queries = x @ self.W_query\n        values = x @ self.W_value\n\n        attn_scores = queries @ keys.T # omega\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1\n        )\n\n        context_vec = attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(123)\nsa_v1 = SelfAttention_v1(d_in, d_out)\nprint(sa_v1(inputs))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2sJFw-QlDtsL","outputId":"aca76206-76c5-4073-f225-4f78cb933cbd","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.161607Z","iopub.execute_input":"2025-05-03T18:19:07.161878Z","iopub.status.idle":"2025-05-03T18:19:07.178980Z","shell.execute_reply.started":"2025-05-03T18:19:07.161860Z","shell.execute_reply":"2025-05-03T18:19:07.178286Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.2996, 0.8053],\n        [0.3061, 0.8210],\n        [0.3058, 0.8203],\n        [0.2948, 0.7939],\n        [0.2927, 0.7891],\n        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n","output_type":"stream"}],"execution_count":80},{"cell_type":"markdown","source":"\n\n*   We can streamline the implementation above using PyTorch's Linear layers, which are equivalent to a matrix multiplication if we disable the bias units\n\n\n","metadata":{"id":"8G0-LB4REnt4"}},{"cell_type":"code","source":"class SelfAttention_v2(nn.Module):\n\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x):\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n\n        context_vec = attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(789)\nsa_v2 = SelfAttention_v2(d_in, d_out)\nprint(sa_v2(inputs))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dA083n_PEkmJ","outputId":"d62533d9-c294-46c7-d864-0b996596cc11","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.179671Z","iopub.execute_input":"2025-05-03T18:19:07.179868Z","iopub.status.idle":"2025-05-03T18:19:07.193118Z","shell.execute_reply.started":"2025-05-03T18:19:07.179854Z","shell.execute_reply":"2025-05-03T18:19:07.192522Z"}},"outputs":[{"name":"stdout","text":"tensor([[-0.0739,  0.0713],\n        [-0.0748,  0.0703],\n        [-0.0749,  0.0702],\n        [-0.0760,  0.0685],\n        [-0.0763,  0.0679],\n        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n","output_type":"stream"}],"execution_count":81},{"cell_type":"markdown","source":"\n\n*   Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they use different initial weights for the weight matrices\n\n","metadata":{"id":"NKl4YCqaEwh8"}},{"cell_type":"markdown","source":"**3.5 Hiding future words with causal attention**\n\n\n\n*   In causal attention, the attention weights above the diagonal are masked, ensuring that for any given input, the LLM is unable to utilize future tokens while calculating the context vectors with the attention weight\n\n\n","metadata":{"id":"np6bSFV-E0cC"}},{"cell_type":"markdown","source":"**3.5.1 Applying a causal attention mask**\n\n\n\n\n*   In this section, we are converting the previous self-attention mechanism into a causal self-attention mechanism\n*   Causal self-attention ensures that the model's prediction for a certain position in a sequence is only dependent on the known outputs at previous positions, not on future positions\n*   To illustrate and implement causal self-attention, let's work with the attention scores and weights from the previous section:\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"fW4w66aQE8NW"}},{"cell_type":"code","source":"# Reuse the query and key weight matrices of the\n# SelfAttention_v2 object from the previous section for convenience\nqueries = sa_v2.W_query(inputs)\nkeys = sa_v2.W_key(inputs)\nattn_scores = queries @ keys.T\n\nattn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\nprint(attn_weights)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KhjOsT3uEuDU","outputId":"7d1aaba8-24db-45eb-8770-256f2bca58e2","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.193984Z","iopub.execute_input":"2025-05-03T18:19:07.194216Z","iopub.status.idle":"2025-05-03T18:19:07.207972Z","shell.execute_reply.started":"2025-05-03T18:19:07.194178Z","shell.execute_reply":"2025-05-03T18:19:07.207321Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}],"execution_count":82},{"cell_type":"markdown","source":"\n\n*   The simplest way to mask out future attention weights is by creating a mask via PyTorch's tril function with elements below the main diagonal (including the diagonal itself) set to 1 and above the main diagonal set to 0:\n\n","metadata":{"id":"HWOAIw5zFhOJ"}},{"cell_type":"code","source":"context_length = attn_scores.shape[0]\nmask_simple = torch.tril(torch.ones(context_length, context_length))\nprint(mask_simple)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vqqFiV_0Fe_3","outputId":"b9e4f37d-1aa6-4d53-da05-90bfdbd31cc2","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.208659Z","iopub.execute_input":"2025-05-03T18:19:07.209187Z","iopub.status.idle":"2025-05-03T18:19:07.222083Z","shell.execute_reply.started":"2025-05-03T18:19:07.209163Z","shell.execute_reply":"2025-05-03T18:19:07.221432Z"}},"outputs":[{"name":"stdout","text":"tensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\n","output_type":"stream"}],"execution_count":83},{"cell_type":"markdown","source":"\n\n*   Then, we can multiply the attention weights with this mask to zero out the attention scores above the diagonal:\n\n","metadata":{"id":"ktRF89M6FmsQ"}},{"cell_type":"code","source":"masked_simple = attn_weights*mask_simple\nprint(masked_simple)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhHQAU2QFk1z","outputId":"97861b20-3ed9-4a1d-afde-d386d8556e78","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.222995Z","iopub.execute_input":"2025-05-03T18:19:07.223253Z","iopub.status.idle":"2025-05-03T18:19:07.234830Z","shell.execute_reply.started":"2025-05-03T18:19:07.223225Z","shell.execute_reply":"2025-05-03T18:19:07.234018Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<MulBackward0>)\n","output_type":"stream"}],"execution_count":84},{"cell_type":"markdown","source":"\n\n*   However, if the mask were applied after softmax, like above, it would disrupt the probability distribution created by softmax\n*   Softmax ensures that all output values sum to 1\n\n\n","metadata":{"id":"Trrs1njOFqZJ"}},{"cell_type":"code","source":"row_sums = masked_simple.sum(dim=-1, keepdim=True)\nmasked_simple_norm = masked_simple / row_sums\nprint(masked_simple_norm)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aeSF5C8IF19z","outputId":"a810a4ec-94c4-4366-e415-cad0ea157b99","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.235688Z","iopub.execute_input":"2025-05-03T18:19:07.235980Z","iopub.status.idle":"2025-05-03T18:19:07.249142Z","shell.execute_reply.started":"2025-05-03T18:19:07.235951Z","shell.execute_reply":"2025-05-03T18:19:07.248363Z"}},"outputs":[{"name":"stdout","text":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<DivBackward0>)\n","output_type":"stream"}],"execution_count":85},{"cell_type":"markdown","source":"\n\n*   While we are technically done with coding the causal attention mechanism now, let's briefly look at a more efficient approach to achieve the same as above\n*   So, instead of zeroing out attention weights above the diagonal and renormalizing the results, we can mask the unnormalized attention scores above the diagonal with negative infinity before they enter the softmax function:\n\n","metadata":{"id":"nQzrg56PF5xS"}},{"cell_type":"code","source":"mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\nprint(masked)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZCRB7XQ5F3qS","outputId":"30768d91-e5b4-4277-a45f-602e19671ac7","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.249865Z","iopub.execute_input":"2025-05-03T18:19:07.250130Z","iopub.status.idle":"2025-05-03T18:19:07.264285Z","shell.execute_reply.started":"2025-05-03T18:19:07.250087Z","shell.execute_reply":"2025-05-03T18:19:07.263617Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n       grad_fn=<MaskedFillBackward0>)\n","output_type":"stream"}],"execution_count":86},{"cell_type":"markdown","source":"\n\n*   As we can see below, now the attention weights in each row correctly sum to 1 again:\n\n","metadata":{"id":"uQ61K_4-GCE8"}},{"cell_type":"code","source":"attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\nprint(attn_weights)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ip4mL-VYF_8v","outputId":"2519bdc5-b41b-431e-d065-b8e85b5cc69a","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.265184Z","iopub.execute_input":"2025-05-03T18:19:07.265540Z","iopub.status.idle":"2025-05-03T18:19:07.277777Z","shell.execute_reply.started":"2025-05-03T18:19:07.265514Z","shell.execute_reply":"2025-05-03T18:19:07.277144Z"}},"outputs":[{"name":"stdout","text":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<SoftmaxBackward0>)\n","output_type":"stream"}],"execution_count":87},{"cell_type":"markdown","source":"**3.5.2 Masking additional attention weights with dropout**\n\n\n\n*   In addition, we also apply dropout to reduce overfitting during training\n*   Dropout can be applied in several places:\n\nfor example, after computing the attention weights;\nor after multiplying the attention weights with the value vectors\n*   Here, we will apply the dropout mask after computing the attention weights because it's more common\n*   Furthermore, in this specific example, we use a dropout rate of 50%, which means randomly masking out half of the attention weights.\n\n\n\n\n","metadata":{"id":"YStWGhSFGHna"}},{"cell_type":"code","source":"torch.manual_seed(123)\ndropout = torch.nn.Dropout(0.5) # dropout rate of 50%\nexample = torch.ones(6, 6) # create a matrix of ones\n\nprint(dropout(example))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jP8sG3jpGFfM","outputId":"3a4de8b1-a0bd-4496-8ca3-23298d9d2bd8","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.278461Z","iopub.execute_input":"2025-05-03T18:19:07.278649Z","iopub.status.idle":"2025-05-03T18:19:07.303660Z","shell.execute_reply.started":"2025-05-03T18:19:07.278635Z","shell.execute_reply":"2025-05-03T18:19:07.303149Z"}},"outputs":[{"name":"stdout","text":"tensor([[2., 2., 2., 2., 2., 2.],\n        [0., 2., 0., 0., 0., 0.],\n        [0., 0., 2., 0., 2., 0.],\n        [2., 2., 0., 0., 0., 2.],\n        [2., 0., 0., 0., 0., 2.],\n        [0., 2., 0., 0., 0., 0.]])\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"torch.manual_seed(123)\nprint(dropout(attn_weights))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AM-Fj6MBGeA9","outputId":"4b8e9627-cb6b-444d-bd4d-b8ee2de57e3e","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.304309Z","iopub.execute_input":"2025-05-03T18:19:07.304551Z","iopub.status.idle":"2025-05-03T18:19:07.309941Z","shell.execute_reply.started":"2025-05-03T18:19:07.304536Z","shell.execute_reply":"2025-05-03T18:19:07.309309Z"}},"outputs":[{"name":"stdout","text":"tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n       grad_fn=<MulBackward0>)\n","output_type":"stream"}],"execution_count":89},{"cell_type":"markdown","source":"\n\n*   Note that the resulting dropout outputs may look different depending on your operating system; you can read more about this inconsistency here on the PyTorch issue tracker\n\n","metadata":{"id":"ojhcyUWjGlyE"}},{"cell_type":"markdown","source":"**3.5.3 Implementing a compact causal self-attention class**\n\n\n\n*   Now, we are ready to implement a working implementation of self-attention, including the causal and dropout masks\n*   For simplicity, to simulate such batch input, we duplicate the input text example:\n\n\n\n\n","metadata":{"id":"WwsL4zMQGqIQ"}},{"cell_type":"code","source":"batch = torch.stack((inputs, inputs), dim=0)\nprint(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71NfHCjLGg24","outputId":"a22d2d68-d6aa-44b4-fe35-c6f2e6b05c48","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.310604Z","iopub.execute_input":"2025-05-03T18:19:07.310830Z","iopub.status.idle":"2025-05-03T18:19:07.322100Z","shell.execute_reply.started":"2025-05-03T18:19:07.310814Z","shell.execute_reply":"2025-05-03T18:19:07.321451Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 6, 3])\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"class CausalAttention(nn.Module):\n\n    def __init__(self, d_in, d_out, context_length,\n                 dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout) # New\n        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape # New batch dimension b\n        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n        # in the mask creation further below.\n        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n        # do not exceed `context_length` before reaching this forward method.\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n        attn_scores.masked_fill_(  # New, _ ops are in-place\n            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1\n        )\n        attn_weights = self.dropout(attn_weights) # New\n\n        context_vec = attn_weights @ values\n        return context_vec\n\ntorch.manual_seed(123)\n\ncontext_length = batch.shape[1]\nca = CausalAttention(d_in, d_out, context_length, 0.0)\n\ncontext_vecs = ca(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRHsD9OFG6F2","outputId":"f59d26de-8909-4e9f-ed4f-6180d05d30ea","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.322758Z","iopub.execute_input":"2025-05-03T18:19:07.323073Z","iopub.status.idle":"2025-05-03T18:19:07.345481Z","shell.execute_reply.started":"2025-05-03T18:19:07.323051Z","shell.execute_reply":"2025-05-03T18:19:07.344844Z"}},"outputs":[{"name":"stdout","text":"tensor([[[-0.4519,  0.2216],\n         [-0.5874,  0.0058],\n         [-0.6300, -0.0632],\n         [-0.5675, -0.0843],\n         [-0.5526, -0.0981],\n         [-0.5299, -0.1081]],\n\n        [[-0.4519,  0.2216],\n         [-0.5874,  0.0058],\n         [-0.6300, -0.0632],\n         [-0.5675, -0.0843],\n         [-0.5526, -0.0981],\n         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 2])\n","output_type":"stream"}],"execution_count":91},{"cell_type":"markdown","source":"\n\n*   Note that dropout is only applied during training, not during inference\n\n","metadata":{"id":"pRvsvjuyHFhd"}},{"cell_type":"markdown","source":"**3.6 Extending single-head attention to multi-head attention**\n\n","metadata":{"id":"TjxMipMzHLRj"}},{"cell_type":"markdown","source":"**3.6.1 Stacking multiple single-head attention layers**\n\n\n\n\n*   Below is a summary of the self-attention implemented previously (causal and dropout masks not shown for simplicity)\n*   This is also called single-head attention:\n\n\n\n\n","metadata":{"id":"miDRq2WCHPSE"}},{"cell_type":"code","source":"class MultiHeadAttentionWrapper(nn.Module):\n\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n             for _ in range(num_heads)]\n        )\n\n    def forward(self, x):\n        return torch.cat([head(x) for head in self.heads], dim=-1)\n\n\ntorch.manual_seed(123)\n\ncontext_length = batch.shape[1] # This is the number of tokens\nd_in, d_out = 3, 2\nmha = MultiHeadAttentionWrapper(\n    d_in, d_out, context_length, 0.0, num_heads=2\n)\n\ncontext_vecs = mha(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_QJolGZG9Qz","outputId":"cb04c338-348a-47df-ea27-9196c3e22f26","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.346360Z","iopub.execute_input":"2025-05-03T18:19:07.346637Z","iopub.status.idle":"2025-05-03T18:19:07.357181Z","shell.execute_reply.started":"2025-05-03T18:19:07.346611Z","shell.execute_reply":"2025-05-03T18:19:07.356467Z"}},"outputs":[{"name":"stdout","text":"tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]],\n\n        [[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 4])\n","output_type":"stream"}],"execution_count":92},{"cell_type":"markdown","source":"\n\n*   In the implementation above, the embedding dimension is 4, because we d_out=2 as the embedding dimension for the key, query, and value vectors as well as the context vector. And since we have 2 attention heads, we have the output embedding dimension 2*2=4\n\n","metadata":{"id":"kwMW3trkHn4D"}},{"cell_type":"markdown","source":"**3.6.2 Implementing multi-head attention with weight splits**\n\n\n\n\n*   While the above is an intuitive and fully functional implementation of multi-head attention (wrapping the single-head attention CausalAttention implementation from earlier), we can write a stand-alone class called MultiHeadAttention to achieve the same\n*   Instead, we create single W_query, W_key, and W_value weight matrices and then split those into individual matrices for each attention head:\n\n\n\n\n","metadata":{"id":"EaWczkSPHqff"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert (d_out % num_heads == 0), \\\n            \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length),\n                       diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,\n        # this will result in errors in the mask creation further below.\n        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n        # do not exceed `context_length` before reaching this forwar\n\n        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n\n        return context_vec\n\ntorch.manual_seed(123)\n\nbatch_size, context_length, d_in = batch.shape\nd_out = 2\nmha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n\ncontext_vecs = mha(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uxi7oY01HlTP","outputId":"d18eee19-cc70-42fa-9376-bb15fe7e8111","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.357921Z","iopub.execute_input":"2025-05-03T18:19:07.358198Z","iopub.status.idle":"2025-05-03T18:19:07.375932Z","shell.execute_reply.started":"2025-05-03T18:19:07.358178Z","shell.execute_reply":"2025-05-03T18:19:07.375230Z"}},"outputs":[{"name":"stdout","text":"tensor([[[0.3190, 0.4858],\n         [0.2943, 0.3897],\n         [0.2856, 0.3593],\n         [0.2693, 0.3873],\n         [0.2639, 0.3928],\n         [0.2575, 0.4028]],\n\n        [[0.3190, 0.4858],\n         [0.2943, 0.3897],\n         [0.2856, 0.3593],\n         [0.2693, 0.3873],\n         [0.2639, 0.3928],\n         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 2])\n","output_type":"stream"}],"execution_count":93},{"cell_type":"markdown","source":"\n\n*   Note that the above is essentially a rewritten version of MultiHeadAttentionWrapper that is more efficient\n*   The resulting output looks a bit different since the random weight initializations differ, but both are fully functional implementations that can be used in the GPT class we will implement in the upcoming chapters\n*   Since the above implementation may look a bit complex at first glance, let's look at what happens when executing attn_scores = queries @ keys.transpose(2, 3):\n\n\n\n\n\n\n\n\n","metadata":{"id":"y6GHnmwsUbhy"}},{"cell_type":"code","source":"# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\na = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n                    [0.8993, 0.0390, 0.9268, 0.7388],\n                    [0.7179, 0.7058, 0.9156, 0.4340]],\n\n                   [[0.0772, 0.3565, 0.1479, 0.5331],\n                    [0.4066, 0.2318, 0.4545, 0.9737],\n                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n\nprint(a @ a.transpose(2, 3))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zp3nckaZH55R","outputId":"532a4ad6-779c-4da7-97cb-37881cea7216","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.376798Z","iopub.execute_input":"2025-05-03T18:19:07.377281Z","iopub.status.idle":"2025-05-03T18:19:07.392712Z","shell.execute_reply.started":"2025-05-03T18:19:07.377257Z","shell.execute_reply":"2025-05-03T18:19:07.392074Z"}},"outputs":[{"name":"stdout","text":"tensor([[[[1.3208, 1.1631, 1.2879],\n          [1.1631, 2.2150, 1.8424],\n          [1.2879, 1.8424, 2.0402]],\n\n         [[0.4391, 0.7003, 0.5903],\n          [0.7003, 1.3737, 1.0620],\n          [0.5903, 1.0620, 0.9912]]]])\n","output_type":"stream"}],"execution_count":94},{"cell_type":"markdown","source":"\n\n*   In this case, the matrix multiplication implementation in PyTorch will handle the 4-dimensional input tensor so that the matrix multiplication is carried out between the 2 last dimensions (num_tokens, head_dim) and then repeated for the individual heads\n\n","metadata":{"id":"q24WLuZOVBac"}},{"cell_type":"code","source":"first_head = a[0, 0, :, :]\nfirst_res = first_head @ first_head.T\nprint(\"First head:\\n\", first_res)\n\nsecond_head = a[0, 1, :, :]\nsecond_res = second_head @ second_head.T\nprint(\"\\nSecond head:\\n\", second_res)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKpEMuDeVAuW","outputId":"52b57d59-2f88-4f12-cba3-2552be2272a3","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.393495Z","iopub.execute_input":"2025-05-03T18:19:07.393779Z","iopub.status.idle":"2025-05-03T18:19:07.411627Z","shell.execute_reply.started":"2025-05-03T18:19:07.393755Z","shell.execute_reply":"2025-05-03T18:19:07.411004Z"}},"outputs":[{"name":"stdout","text":"First head:\n tensor([[1.3208, 1.1631, 1.2879],\n        [1.1631, 2.2150, 1.8424],\n        [1.2879, 1.8424, 2.0402]])\n\nSecond head:\n tensor([[0.4391, 0.7003, 0.5903],\n        [0.7003, 1.3737, 1.0620],\n        [0.5903, 1.0620, 0.9912]])\n","output_type":"stream"}],"execution_count":95},{"cell_type":"markdown","source":"**Multi-head Attention Plus Data Loading**","metadata":{"id":"e4QZVuSfVSg7"}},{"cell_type":"code","source":"# NBVAL_IGNORE_OUTPUT\nfrom importlib.metadata import version\n\nprint(\"torch version:\", version(\"torch\"))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bwbz9P96VJp1","outputId":"31a30c0e-ca3b-4c4a-f855-f7e1507b9dbe","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.412424Z","iopub.execute_input":"2025-05-03T18:19:07.412910Z","iopub.status.idle":"2025-05-03T18:19:07.424739Z","shell.execute_reply.started":"2025-05-03T18:19:07.412891Z","shell.execute_reply":"2025-05-03T18:19:07.424060Z"}},"outputs":[{"name":"stdout","text":"torch version: 2.5.1+cu124\n","output_type":"stream"}],"execution_count":96},{"cell_type":"markdown","source":"\n\n*   This notebook contains the main takeaway, multihead-attention implementation (plus the data loading pipeline from chapter 2)\n\n","metadata":{"id":"dqSHvNNnVZVR"}},{"cell_type":"markdown","source":"**Data Loader from Chapter 2**","metadata":{"id":"pZTtCmy9Vddp"}},{"cell_type":"code","source":"import tiktoken\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n\n        # Tokenize the entire text\n        token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n\n        # Use a sliding window to chunk the book into overlapping sequences of max_length\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n\n\ndef create_dataloader(txt, batch_size=4, max_length=256, stride=128, shuffle=True):\n    # Initialize the tokenizer\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n\n    # Create dataset\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n\n    # Create dataloader\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n    return dataloader\n\n\nwith open(\"/kaggle/input/major-project/small-text-sample.txt\", \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\nencoded_text = tokenizer.encode(raw_text)\n\nvocab_size = 50257\noutput_dim = 256\nmax_len = 1024\ncontext_length = max_len\n\n\ntoken_embedding_layer = nn.Embedding(vocab_size, output_dim)\npos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n\nmax_length = 4\ndataloader = create_dataloader(raw_text, batch_size=8, max_length=max_length, stride=max_length)","metadata":{"id":"QvPlMuaVVWov","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.425512Z","iopub.execute_input":"2025-05-03T18:19:07.425773Z","iopub.status.idle":"2025-05-03T18:19:07.567481Z","shell.execute_reply.started":"2025-05-03T18:19:07.425758Z","shell.execute_reply":"2025-05-03T18:19:07.566896Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"for batch in dataloader:\n    x, y = batch\n\n    token_embeddings = token_embedding_layer(x)\n    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n\n    input_embeddings = token_embeddings + pos_embeddings\n\n    break","metadata":{"id":"aGOhkzvHVkOB","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.568463Z","iopub.execute_input":"2025-05-03T18:19:07.568649Z","iopub.status.idle":"2025-05-03T18:19:07.578860Z","shell.execute_reply.started":"2025-05-03T18:19:07.568633Z","shell.execute_reply":"2025-05-03T18:19:07.578172Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"print(input_embeddings.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeWCREcDWUgW","outputId":"24edc079-61fa-4aac-dd93-16ba02865710","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.579611Z","iopub.execute_input":"2025-05-03T18:19:07.579853Z","iopub.status.idle":"2025-05-03T18:19:07.586576Z","shell.execute_reply.started":"2025-05-03T18:19:07.579832Z","shell.execute_reply":"2025-05-03T18:19:07.585957Z"}},"outputs":[{"name":"stdout","text":"torch.Size([8, 4, 256])\n","output_type":"stream"}],"execution_count":99},{"cell_type":"markdown","source":"**Multi-head Attention from Chapter 3**","metadata":{"id":"lzHruzd6WYQq"}},{"cell_type":"markdown","source":"**Variant A: Simple implementation**","metadata":{"id":"JZMe8dThWcW9"}},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n\n    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout) # New\n        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n\n    def forward(self, x):\n        b, n_tokens, d_in = x.shape # New batch dimension b\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n        attn_scores.masked_fill_(  # New, _ ops are in-place\n            self.mask.bool()[:n_tokens, :n_tokens], -torch.inf)\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights) # New\n\n        context_vec = attn_weights @ values\n        return context_vec\n\n\nclass MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [CausalSelfAttention(d_in, d_out, context_length, dropout, qkv_bias)\n             for _ in range(num_heads)]\n        )\n        self.out_proj = nn.Linear(d_out*num_heads, d_out*num_heads)\n\n    def forward(self, x):\n        context_vec = torch.cat([head(x) for head in self.heads], dim=-1)\n        return self.out_proj(context_vec)","metadata":{"id":"cK0NGMveWXrs","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.587340Z","iopub.execute_input":"2025-05-03T18:19:07.587513Z","iopub.status.idle":"2025-05-03T18:19:07.608387Z","shell.execute_reply.started":"2025-05-03T18:19:07.587499Z","shell.execute_reply":"2025-05-03T18:19:07.607637Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"torch.manual_seed(123)\n\ncontext_length = max_length\nd_in = output_dim\n\nnum_heads=2\nd_out = d_in // num_heads\n\nmha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads)\n\nbatch = input_embeddings\ncontext_vecs = mha(batch)\n\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vjXAnM7LWiQO","outputId":"43eabf46-1b36-4836-9c20-e3c2bf04bcc6","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.609308Z","iopub.execute_input":"2025-05-03T18:19:07.609516Z","iopub.status.idle":"2025-05-03T18:19:07.631256Z","shell.execute_reply.started":"2025-05-03T18:19:07.609500Z","shell.execute_reply":"2025-05-03T18:19:07.630624Z"}},"outputs":[{"name":"stdout","text":"context_vecs.shape: torch.Size([8, 4, 256])\n","output_type":"stream"}],"execution_count":101},{"cell_type":"markdown","source":"**Variant B: Alternative implementation**","metadata":{"id":"7SKPrkg2WlMp"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) # optional projection\n\n        return context_vec","metadata":{"id":"HD3n6EgAWkjx","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.632207Z","iopub.execute_input":"2025-05-03T18:19:07.632463Z","iopub.status.idle":"2025-05-03T18:19:07.640817Z","shell.execute_reply.started":"2025-05-03T18:19:07.632438Z","shell.execute_reply":"2025-05-03T18:19:07.640172Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"torch.manual_seed(123)\n\ncontext_length = max_length\nd_in = output_dim\nd_out = d_in\n\nmha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n\nbatch = input_embeddings\ncontext_vecs = mha(batch)\n\nprint(\"context_vecs.shape:\", context_vecs.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkMFzmuKWvMy","outputId":"cbbc11f5-f7d4-4008-921a-6f12e9143616","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.641623Z","iopub.execute_input":"2025-05-03T18:19:07.642094Z","iopub.status.idle":"2025-05-03T18:19:07.663176Z","shell.execute_reply.started":"2025-05-03T18:19:07.642070Z","shell.execute_reply":"2025-05-03T18:19:07.662283Z"}},"outputs":[{"name":"stdout","text":"context_vecs.shape: torch.Size([8, 4, 256])\n","output_type":"stream"}],"execution_count":103},{"cell_type":"markdown","source":"\n\n---\n\n\n\n---\n\n","metadata":{"id":"0XavsVTaXG9N"}},{"cell_type":"markdown","source":"**Chapter 4: Implementing a GPT model from Scratch To Generate Text**","metadata":{"id":"wLIu-EtdXNAL"}},{"cell_type":"code","source":"from importlib.metadata import version\n\nprint(\"matplotlib version:\", version(\"matplotlib\"))\nprint(\"torch version:\", version(\"torch\"))\nprint(\"tiktoken version:\", version(\"tiktoken\"))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDSw6R85WyQF","outputId":"564fd776-a263-45a4-9d0f-31acf4adbe19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.663956Z","iopub.execute_input":"2025-05-03T18:19:07.664266Z","iopub.status.idle":"2025-05-03T18:19:07.671613Z","shell.execute_reply.started":"2025-05-03T18:19:07.664242Z","shell.execute_reply":"2025-05-03T18:19:07.670934Z"}},"outputs":[{"name":"stdout","text":"matplotlib version: 3.7.5\ntorch version: 2.5.1+cu124\ntiktoken version: 0.9.0\n","output_type":"stream"}],"execution_count":104},{"cell_type":"markdown","source":"\n\n*   In this chapter, we implement a GPT-like LLM architecture; the next chapter will focus on training this LLM\n\n","metadata":{"id":"sS1WGrJLXwDG"}},{"cell_type":"markdown","source":"**4.1 Coding an LLM architecture**\n\n\n\n*   Chapter 1 discussed models like GPT and Llama, which generate words sequentially and are based on the decoder part of the original transformer architecture\n*   Therefore, these LLMs are often referred to as \"decoder-like\" LLMs\n*   In this chapter, we consider embedding and model sizes akin to a small GPT-2 model\n*   We'll specifically code the architecture of the smallest GPT-2 model, as outlined in Radford et al.'s Language Models are Unsupervised Multitask Learners\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"-5_tuoC1X1wR"}},{"cell_type":"code","source":"GPT_CONFIG_124M = {\n    \"vocab_size\": 50257,    # Vocabulary size\n    \"context_length\": 1024, # Context length\n    \"emb_dim\": 768,         # Embedding dimension\n    \"n_heads\": 12,          # Number of attention heads\n    \"n_layers\": 12,         # Number of layers\n    \"drop_rate\": 0.1,       # Dropout rate\n    \"qkv_bias\": False       # Query-Key-Value bias\n}","metadata":{"id":"4DqHKsqNXtxz","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.672406Z","iopub.execute_input":"2025-05-03T18:19:07.672680Z","iopub.status.idle":"2025-05-03T18:19:07.680835Z","shell.execute_reply.started":"2025-05-03T18:19:07.672658Z","shell.execute_reply":"2025-05-03T18:19:07.680271Z"}},"outputs":[],"execution_count":105},{"cell_type":"markdown","source":"\n\n*   We use short variable names to avoid long lines of code later\n*   \"vocab_size\" indicates a vocabulary size of 50,257 words, supported by the BPE tokenizer discussed in Chapter 2\n*   \"drop_rate\" is the dropout mechanism's intensity, discussed in Chapter 3; 0.1 means dropping 10% of hidden units during training to mitigate overfitting\n*   \"qkv_bias\" decides if the Linear layers in the multi-head attention mechanism (from Chapter 3) should include a bias vector when computing query (Q), key (K), and value (V) tensors; we'll disable this option, which is standard practice in modern LLMs; however, we'll revisit this later when loading pretrained GPT-2 weights from OpenAI into our reimplementation in chapter 5\n\n\n\n\n\n\n\n","metadata":{"id":"hXCRy6KWYpRv"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass DummyGPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        # Use a placeholder for TransformerBlock\n        self.trf_blocks = nn.Sequential(\n            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n        # Use a placeholder for LayerNorm\n        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits\n\n\nclass DummyTransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        # A simple placeholder\n\n    def forward(self, x):\n        # This block does nothing and just returns its input.\n        return x\n\n\nclass DummyLayerNorm(nn.Module):\n    def __init__(self, normalized_shape, eps=1e-5):\n        super().__init__()\n        # The parameters here are just to mimic the LayerNorm interface.\n\n    def forward(self, x):\n        # This layer does nothing and just returns its input.\n        return x","metadata":{"id":"-XMMegOuYmzx","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.681504Z","iopub.execute_input":"2025-05-03T18:19:07.681669Z","iopub.status.idle":"2025-05-03T18:19:07.691049Z","shell.execute_reply.started":"2025-05-03T18:19:07.681655Z","shell.execute_reply":"2025-05-03T18:19:07.690474Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"import tiktoken\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\nbatch = []\n\ntxt1 = \"Every effort moves you\"\ntxt2 = \"Every day holds a\"\n\nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch = torch.stack(batch, dim=0)\nprint(batch)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g5yXGEBxZHvU","outputId":"4f958fc4-00cc-47c2-811e-e236e0314989","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.691892Z","iopub.execute_input":"2025-05-03T18:19:07.692142Z","iopub.status.idle":"2025-05-03T18:19:07.709752Z","shell.execute_reply.started":"2025-05-03T18:19:07.692118Z","shell.execute_reply":"2025-05-03T18:19:07.709064Z"}},"outputs":[{"name":"stdout","text":"tensor([[6109, 3626, 6100,  345],\n        [6109, 1110, 6622,  257]])\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"torch.manual_seed(123)\nmodel = DummyGPTModel(GPT_CONFIG_124M)\n\nlogits = model(batch)\nprint(\"Output shape:\", logits.shape)\nprint(logits)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oR7iFNGqZZmH","outputId":"ea7db03b-155d-4dac-857c-856306d0d9af","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:07.710508Z","iopub.execute_input":"2025-05-03T18:19:07.710720Z","iopub.status.idle":"2025-05-03T18:19:08.424567Z","shell.execute_reply.started":"2025-05-03T18:19:07.710703Z","shell.execute_reply":"2025-05-03T18:19:08.423863Z"}},"outputs":[{"name":"stdout","text":"Output shape: torch.Size([2, 4, 50257])\ntensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n         [ 0.0448,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n\n        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n       grad_fn=<UnsafeViewBackward0>)\n","output_type":"stream"}],"execution_count":108},{"cell_type":"markdown","source":"**4.2 Normalizing activations with layer normalization**\n\n\n\n*   Layer normalization, also known as LayerNorm (Ba et al. 2016), centers the activations of a neural network layer around a mean of 0 and normalizes their variance to 1\n*   Layer normalization is applied both before and after the multi-head attention module within the transformer block, which we will implement later; it's also applied before the final output layer\n\n\n\n","metadata":{"id":"6P_DUimAZh9X"}},{"cell_type":"code","source":"torch.manual_seed(123)\n\n# create 2 training examples with 5 dimensions (features) each\nbatch_example = torch.randn(2, 5)\n\nlayer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\nout = layer(batch_example)\nprint(out)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rtohmw4cZc3j","outputId":"93a4980b-49a2-4ce4-f2c8-ea43ed582371","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.425327Z","iopub.execute_input":"2025-05-03T18:19:08.426033Z","iopub.status.idle":"2025-05-03T18:19:08.433482Z","shell.execute_reply.started":"2025-05-03T18:19:08.426012Z","shell.execute_reply":"2025-05-03T18:19:08.432778Z"}},"outputs":[{"name":"stdout","text":"tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n       grad_fn=<ReluBackward0>)\n","output_type":"stream"}],"execution_count":109},{"cell_type":"markdown","source":"\n\n*   Let's compute the mean and variance for each of the 2 inputs above:\n\n","metadata":{"id":"yEyAFJFeZ2rg"}},{"cell_type":"code","source":"mean = out.mean(dim=-1, keepdim=True)\nvar = out.var(dim=-1, keepdim=True)\n\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcL4-V3HZ0AA","outputId":"38abcfa8-708b-4e0b-cabd-5431c545bf42","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.434324Z","iopub.execute_input":"2025-05-03T18:19:08.434589Z","iopub.status.idle":"2025-05-03T18:19:08.455968Z","shell.execute_reply.started":"2025-05-03T18:19:08.434568Z","shell.execute_reply":"2025-05-03T18:19:08.454931Z"}},"outputs":[{"name":"stdout","text":"Mean:\n tensor([[0.1324],\n        [0.2170]], grad_fn=<MeanBackward1>)\nVariance:\n tensor([[0.0231],\n        [0.0398]], grad_fn=<VarBackward0>)\n","output_type":"stream"}],"execution_count":110},{"cell_type":"markdown","source":"\n\n*   The normalization is applied to each of the two inputs (rows) independently; using dim=-1 applies the calculation across the last dimension (in this case, the feature dimension) instead of the row dimension\n\n\n\n\n\n","metadata":{"id":"Mue-v1rWZ9A1"}},{"cell_type":"code","source":"out_norm = (out - mean) / torch.sqrt(var)\nprint(\"Normalized layer outputs:\\n\", out_norm)\n\nmean = out_norm.mean(dim=-1, keepdim=True)\nvar = out_norm.var(dim=-1, keepdim=True)\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqpcPmvtZ6YJ","outputId":"a66991cf-ba89-416f-a27f-f9a320807cc4","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.457509Z","iopub.execute_input":"2025-05-03T18:19:08.458288Z","iopub.status.idle":"2025-05-03T18:19:08.466566Z","shell.execute_reply.started":"2025-05-03T18:19:08.458257Z","shell.execute_reply":"2025-05-03T18:19:08.465761Z"}},"outputs":[{"name":"stdout","text":"Normalized layer outputs:\n tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n       grad_fn=<DivBackward0>)\nMean:\n tensor([[9.9341e-09],\n        [1.9868e-08]], grad_fn=<MeanBackward1>)\nVariance:\n tensor([[1.0000],\n        [1.0000]], grad_fn=<VarBackward0>)\n","output_type":"stream"}],"execution_count":111},{"cell_type":"markdown","source":"\n\n*   Each input is centered at 0 and has a unit variance of 1; to improve readability, we can disable PyTorch's scientific notation:\n\n","metadata":{"id":"BCzDrwXAaKPH"}},{"cell_type":"code","source":"torch.set_printoptions(sci_mode=False)\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hQcEPLsaHpC","outputId":"33b7ac76-3230-4667-da32-f88959569003","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.467389Z","iopub.execute_input":"2025-05-03T18:19:08.467689Z","iopub.status.idle":"2025-05-03T18:19:08.482579Z","shell.execute_reply.started":"2025-05-03T18:19:08.467663Z","shell.execute_reply":"2025-05-03T18:19:08.481371Z"}},"outputs":[{"name":"stdout","text":"Mean:\n tensor([[    0.0000],\n        [    0.0000]], grad_fn=<MeanBackward1>)\nVariance:\n tensor([[1.0000],\n        [1.0000]], grad_fn=<VarBackward0>)\n","output_type":"stream"}],"execution_count":112},{"cell_type":"markdown","source":"\n\n*   Above, we normalized the features of each input\n*   Now, using the same idea, we can implement a LayerNorm class:\n\n","metadata":{"id":"CJTfoh0UaQXn"}},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift","metadata":{"id":"mWi7e594aOIv","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.483659Z","iopub.execute_input":"2025-05-03T18:19:08.483955Z","iopub.status.idle":"2025-05-03T18:19:08.496869Z","shell.execute_reply.started":"2025-05-03T18:19:08.483930Z","shell.execute_reply":"2025-05-03T18:19:08.496031Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"ln = LayerNorm(emb_dim=5)\nout_ln = ln(batch_example)","metadata":{"id":"Uca8VFNoaasK","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.497666Z","iopub.execute_input":"2025-05-03T18:19:08.497915Z","iopub.status.idle":"2025-05-03T18:19:08.510198Z","shell.execute_reply.started":"2025-05-03T18:19:08.497893Z","shell.execute_reply":"2025-05-03T18:19:08.509521Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"mean = out_ln.mean(dim=-1, keepdim=True)\nvar = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n\nprint(\"Mean:\\n\", mean)\nprint(\"Variance:\\n\", var)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07AcUAvoqjaX","outputId":"13b035e8-1fb9-4894-b216-67b66c3cf6aa","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.510925Z","iopub.execute_input":"2025-05-03T18:19:08.511225Z","iopub.status.idle":"2025-05-03T18:19:08.525890Z","shell.execute_reply.started":"2025-05-03T18:19:08.511200Z","shell.execute_reply":"2025-05-03T18:19:08.525046Z"}},"outputs":[{"name":"stdout","text":"Mean:\n tensor([[    -0.0000],\n        [     0.0000]], grad_fn=<MeanBackward1>)\nVariance:\n tensor([[1.0000],\n        [1.0000]], grad_fn=<VarBackward0>)\n","output_type":"stream"}],"execution_count":115},{"cell_type":"markdown","source":"**4.3 Implementing a feed forward network with GELU activations**\n\n\n\n*   In this section, we implement a small neural network submodule that is used as part of the transformer block in LLMs\n*   We start with the activation function\n\n","metadata":{"id":"_e1zUaFwsfIY"}},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n            (x + 0.044715 * torch.pow(x, 3))\n        ))","metadata":{"id":"xAWu76kwql1k","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.527044Z","iopub.execute_input":"2025-05-03T18:19:08.527888Z","iopub.status.idle":"2025-05-03T18:19:08.537311Z","shell.execute_reply.started":"2025-05-03T18:19:08.527859Z","shell.execute_reply":"2025-05-03T18:19:08.536674Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ngelu, relu = GELU(), nn.ReLU()\n\n# Some sample data\nx = torch.linspace(-3, 3, 100)\ny_gelu, y_relu = gelu(x), relu(x)\n\nplt.figure(figsize=(8, 3))\nfor i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n    plt.subplot(1, 2, i)\n    plt.plot(x, y)\n    plt.title(f\"{label} activation function\")\n    plt.xlabel(\"x\")\n    plt.ylabel(f\"{label}(x)\")\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"id":"l1oZNKJLst9K","outputId":"9c79103b-ddf7-4eaa-f8b8-56ab12e57a37","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.537984Z","iopub.execute_input":"2025-05-03T18:19:08.538241Z","iopub.status.idle":"2025-05-03T18:19:08.919799Z","shell.execute_reply.started":"2025-05-03T18:19:08.538219Z","shell.execute_reply":"2025-05-03T18:19:08.919084Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x300 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":117},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)","metadata":{"id":"n2Wd51p8swsB","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.920632Z","iopub.execute_input":"2025-05-03T18:19:08.920897Z","iopub.status.idle":"2025-05-03T18:19:08.925328Z","shell.execute_reply.started":"2025-05-03T18:19:08.920874Z","shell.execute_reply":"2025-05-03T18:19:08.924524Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"print(GPT_CONFIG_124M[\"emb_dim\"])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9TEUaJRos0kc","outputId":"aa044860-079c-4501-df6c-1d613b3910b0","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.926171Z","iopub.execute_input":"2025-05-03T18:19:08.926376Z","iopub.status.idle":"2025-05-03T18:19:08.940993Z","shell.execute_reply.started":"2025-05-03T18:19:08.926359Z","shell.execute_reply":"2025-05-03T18:19:08.940447Z"}},"outputs":[{"name":"stdout","text":"768\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"ffn = FeedForward(GPT_CONFIG_124M)\n\n# input shape: [batch_size, num_token, emb_size]\nx = torch.rand(2, 3, 768)\nout = ffn(x)\nprint(out.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9UM3HKCSs29y","outputId":"32c6e5ba-e5ef-4dea-c9ff-fd2e6750a816","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.941744Z","iopub.execute_input":"2025-05-03T18:19:08.941974Z","iopub.status.idle":"2025-05-03T18:19:08.998076Z","shell.execute_reply.started":"2025-05-03T18:19:08.941950Z","shell.execute_reply":"2025-05-03T18:19:08.997371Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 3, 768])\n","output_type":"stream"}],"execution_count":120},{"cell_type":"markdown","source":"**4.4 Adding shortcut connections**","metadata":{"id":"fQ6Bya2Ms8XT"}},{"cell_type":"code","source":"class ExampleDeepNeuralNetwork(nn.Module):\n    def __init__(self, layer_sizes, use_shortcut):\n        super().__init__()\n        self.use_shortcut = use_shortcut\n        self.layers = nn.ModuleList([\n            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n        ])\n\n    def forward(self, x):\n        for layer in self.layers:\n            # Compute the output of the current layer\n            layer_output = layer(x)\n            # Check if shortcut can be applied\n            if self.use_shortcut and x.shape == layer_output.shape:\n                x = x + layer_output\n            else:\n                x = layer_output\n        return x\n\n\ndef print_gradients(model, x):\n    # Forward pass\n    output = model(x)\n    target = torch.tensor([[0.]])\n\n    # Calculate loss based on how close the target\n    # and output are\n    loss = nn.MSELoss()\n    loss = loss(output, target)\n\n    # Backward pass to calculate the gradients\n    loss.backward()\n\n    for name, param in model.named_parameters():\n        if 'weight' in name:\n            # Print the mean absolute gradient of the weights\n            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")","metadata":{"id":"05z-pGQ_s5T4","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:08.998932Z","iopub.execute_input":"2025-05-03T18:19:08.999235Z","iopub.status.idle":"2025-05-03T18:19:09.007431Z","shell.execute_reply.started":"2025-05-03T18:19:08.999217Z","shell.execute_reply":"2025-05-03T18:19:09.006793Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"layer_sizes = [3, 3, 3, 3, 3, 1]\n\nsample_input = torch.tensor([[1., 0., -1.]])\n\ntorch.manual_seed(123)\nmodel_without_shortcut = ExampleDeepNeuralNetwork(\n    layer_sizes, use_shortcut=False\n)\nprint_gradients(model_without_shortcut, sample_input)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhRQDpOMtB3E","outputId":"f22f6fa6-da5a-465c-baaf-df0f8f1715b7","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:09.017294Z","iopub.execute_input":"2025-05-03T18:19:09.018030Z","iopub.status.idle":"2025-05-03T18:19:09.095199Z","shell.execute_reply.started":"2025-05-03T18:19:09.018003Z","shell.execute_reply":"2025-05-03T18:19:09.094401Z"}},"outputs":[{"name":"stdout","text":"layers.0.0.weight has gradient mean of 0.00020173584925942123\nlayers.1.0.weight has gradient mean of 0.00012011159560643137\nlayers.2.0.weight has gradient mean of 0.0007152040489017963\nlayers.3.0.weight has gradient mean of 0.0013988736318424344\nlayers.4.0.weight has gradient mean of 0.005049645435065031\n","output_type":"stream"}],"execution_count":122},{"cell_type":"code","source":"torch.manual_seed(123)\nmodel_with_shortcut = ExampleDeepNeuralNetwork(\n    layer_sizes, use_shortcut=True\n)\nprint_gradients(model_with_shortcut, sample_input)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H9YGFWE3tEUU","outputId":"ca215643-a0f9-4045-9474-bc944568ba6b","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:09.096205Z","iopub.execute_input":"2025-05-03T18:19:09.096479Z","iopub.status.idle":"2025-05-03T18:19:09.105270Z","shell.execute_reply.started":"2025-05-03T18:19:09.096451Z","shell.execute_reply":"2025-05-03T18:19:09.104666Z"}},"outputs":[{"name":"stdout","text":"layers.0.0.weight has gradient mean of 0.22169792652130127\nlayers.1.0.weight has gradient mean of 0.20694108307361603\nlayers.2.0.weight has gradient mean of 0.3289699852466583\nlayers.3.0.weight has gradient mean of 0.2665732204914093\nlayers.4.0.weight has gradient mean of 1.3258541822433472\n","output_type":"stream"}],"execution_count":123},{"cell_type":"markdown","source":"**4.5 Connecting attention and linear layers in a transformer block**","metadata":{"id":"-mv7em62tJBX"}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Path to the working directory\nworking_dir = '/kaggle/working/'\n\n# Check if the directory exists\nif os.path.exists(working_dir):\n    # Loop through the directory and remove all files\n    for filename in os.listdir(working_dir):\n        file_path = os.path.join(working_dir, filename)\n        \n        # If it's a file, delete it\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n        # If it's a directory, delete the directory\n        elif os.path.isdir(file_path):\n            shutil.rmtree(file_path)\n    print(\"All files in the working directory have been deleted.\")\nelse:\n    print(\"The working directory does not exist.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:09.106124Z","iopub.execute_input":"2025-05-03T18:19:09.106338Z","iopub.status.idle":"2025-05-03T18:19:09.116696Z","shell.execute_reply.started":"2025-05-03T18:19:09.106323Z","shell.execute_reply":"2025-05-03T18:19:09.116068Z"}},"outputs":[{"name":"stdout","text":"All files in the working directory have been deleted.\n","output_type":"stream"}],"execution_count":124},{"cell_type":"code","source":"import os\nimport shutil\n\n# Source and destination directories\nsrc_dir = \"/kaggle/input/major-project/\"\ndst_dir = \"/kaggle/working/\"\n\n# Copy all files from source to destination\nfor root, dirs, files in os.walk(src_dir):\n    for file in files:\n        full_src_path = os.path.join(root, file)\n        dst_path = os.path.join(dst_dir, file)\n        shutil.copy(full_src_path, dst_path)\n        # print(f\"Copied: {file} -> {dst_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:09.117393Z","iopub.execute_input":"2025-05-03T18:19:09.117595Z","iopub.status.idle":"2025-05-03T18:19:09.972692Z","shell.execute_reply.started":"2025-05-03T18:19:09.117575Z","shell.execute_reply":"2025-05-03T18:19:09.972080Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"# from previous_chapters import MultiHeadAttention\nfrom previous_chapters import MultiHeadAttention\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in=cfg[\"emb_dim\"],\n            d_out=cfg[\"emb_dim\"],\n            context_length=cfg[\"context_length\"],\n            num_heads=cfg[\"n_heads\"],\n            dropout=cfg[\"drop_rate\"],\n            qkv_bias=cfg[\"qkv_bias\"])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n\n    def forward(self, x):\n        # Shortcut connection for attention block\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n        x = self.drop_shortcut(x)\n        x = x + shortcut  # Add the original input back\n\n        # Shortcut connection for feed forward block\n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut  # Add the original input back\n\n        return x","metadata":{"id":"A2rumpFctGXM","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:09.973409Z","iopub.execute_input":"2025-05-03T18:19:09.973602Z","iopub.status.idle":"2025-05-03T18:19:09.981965Z","shell.execute_reply.started":"2025-05-03T18:19:09.973587Z","shell.execute_reply":"2025-05-03T18:19:09.981371Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"torch.manual_seed(123)\n\nx = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\nblock = TransformerBlock(GPT_CONFIG_124M)\noutput = block(x)\n\nprint(\"Input shape:\", x.shape)\nprint(\"Output shape:\", output.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-isqAUhxtM1K","outputId":"443b83b3-b9bb-482f-e59f-829ce649c961","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:09.982586Z","iopub.execute_input":"2025-05-03T18:19:09.982765Z","iopub.status.idle":"2025-05-03T18:19:10.074368Z","shell.execute_reply.started":"2025-05-03T18:19:09.982750Z","shell.execute_reply":"2025-05-03T18:19:10.073664Z"}},"outputs":[{"name":"stdout","text":"Input shape: torch.Size([2, 4, 768])\nOutput shape: torch.Size([2, 4, 768])\n","output_type":"stream"}],"execution_count":127},{"cell_type":"markdown","source":"**4.6 Coding the GPT model**","metadata":{"id":"T7XaR9X8uKiD"}},{"cell_type":"code","source":"class GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits","metadata":{"id":"0ijxNAlNuHem","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:10.075175Z","iopub.execute_input":"2025-05-03T18:19:10.075441Z","iopub.status.idle":"2025-05-03T18:19:10.081307Z","shell.execute_reply.started":"2025-05-03T18:19:10.075414Z","shell.execute_reply":"2025-05-03T18:19:10.080543Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"torch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\n\nout = model(batch)\nprint(\"Input batch:\\n\", batch)\nprint(\"\\nOutput shape:\", out.shape)\nprint(out)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A8TZTdbMuPsm","outputId":"4c335748-1dfe-49d7-9286-22205b92fa1e","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:10.082100Z","iopub.execute_input":"2025-05-03T18:19:10.082503Z","iopub.status.idle":"2025-05-03T18:19:11.608737Z","shell.execute_reply.started":"2025-05-03T18:19:10.082484Z","shell.execute_reply":"2025-05-03T18:19:11.607943Z"}},"outputs":[{"name":"stdout","text":"Input batch:\n tensor([[6109, 3626, 6100,  345],\n        [6109, 1110, 6622,  257]])\n\nOutput shape: torch.Size([2, 4, 50257])\ntensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n\n        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n       grad_fn=<UnsafeViewBackward0>)\n","output_type":"stream"}],"execution_count":129},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params:,}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUBE66_EuRgY","outputId":"cfd5dd23-0f7a-40e7-b2e0-84d9e57a7d0c","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:11.609558Z","iopub.execute_input":"2025-05-03T18:19:11.609850Z","iopub.status.idle":"2025-05-03T18:19:11.615131Z","shell.execute_reply.started":"2025-05-03T18:19:11.609824Z","shell.execute_reply":"2025-05-03T18:19:11.614417Z"}},"outputs":[{"name":"stdout","text":"Total number of parameters: 163,009,536\n","output_type":"stream"}],"execution_count":130},{"cell_type":"code","source":"print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\nprint(\"Output layer shape:\", model.out_head.weight.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJ4YM4-LuUoZ","outputId":"bffd658e-8fd9-4554-b6ba-1d1e1649e209","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:11.615987Z","iopub.execute_input":"2025-05-03T18:19:11.616219Z","iopub.status.idle":"2025-05-03T18:19:11.632827Z","shell.execute_reply.started":"2025-05-03T18:19:11.616202Z","shell.execute_reply":"2025-05-03T18:19:11.632001Z"}},"outputs":[{"name":"stdout","text":"Token embedding layer shape: torch.Size([50257, 768])\nOutput layer shape: torch.Size([50257, 768])\n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\nprint(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RjbmDEmMuWzz","outputId":"cfbb4016-ce2e-40af-c89b-9055dcf22049","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:11.633692Z","iopub.execute_input":"2025-05-03T18:19:11.634092Z","iopub.status.idle":"2025-05-03T18:19:11.647916Z","shell.execute_reply.started":"2025-05-03T18:19:11.634075Z","shell.execute_reply":"2025-05-03T18:19:11.647335Z"}},"outputs":[{"name":"stdout","text":"Number of trainable parameters considering weight tying: 124,412,160\n","output_type":"stream"}],"execution_count":132},{"cell_type":"code","source":"# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\ntotal_size_bytes = total_params * 4\n\n# Convert to megabytes\ntotal_size_mb = total_size_bytes / (1024 * 1024)\n\nprint(f\"Total size of the model: {total_size_mb:.2f} MB\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sD3_hlluYuq","outputId":"dfd47b66-9f1b-48f6-c0a7-61ff633d0d5a","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:11.648682Z","iopub.execute_input":"2025-05-03T18:19:11.648921Z","iopub.status.idle":"2025-05-03T18:19:11.662691Z","shell.execute_reply.started":"2025-05-03T18:19:11.648900Z","shell.execute_reply":"2025-05-03T18:19:11.662154Z"}},"outputs":[{"name":"stdout","text":"Total size of the model: 621.83 MB\n","output_type":"stream"}],"execution_count":133},{"cell_type":"markdown","source":"**4.7 Generating text**","metadata":{"id":"mxtYIn4eueJC"}},{"cell_type":"code","source":"def generate_text_simple(model, idx, max_new_tokens, context_size):\n    # idx is (batch, n_tokens) array of indices in the current context\n    for _ in range(max_new_tokens):\n\n        # Crop current context if it exceeds the supported context size\n        # E.g., if LLM supports only 5 tokens, and the context size is 10\n        # then only the last 5 tokens are used as context\n        idx_cond = idx[:, -context_size:]\n\n        # Get the predictions\n        with torch.no_grad():\n            logits = model(idx_cond)\n\n        # Focus only on the last time step\n        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n        logits = logits[:, -1, :]\n\n        # Apply softmax to get probabilities\n        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n\n        # Get the idx of the vocab entry with the highest probability value\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n\n        # Append sampled index to the running sequence\n        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n\n    return idx","metadata":{"id":"Hz8DAnRmubFn","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:11.663373Z","iopub.execute_input":"2025-05-03T18:19:11.663611Z","iopub.status.idle":"2025-05-03T18:19:11.676642Z","shell.execute_reply.started":"2025-05-03T18:19:11.663590Z","shell.execute_reply":"2025-05-03T18:19:11.676009Z"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"start_context = \"Hello, I am\"\n\nencoded = tokenizer.encode(start_context)\nprint(\"encoded:\", encoded)\n\nencoded_tensor = torch.tensor(encoded).unsqueeze(0)\nprint(\"encoded_tensor.shape:\", encoded_tensor.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tV9yi0-auhoj","outputId":"62243435-2d7c-401b-eb3d-874f6427739e","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:11.677261Z","iopub.execute_input":"2025-05-03T18:19:11.677482Z","iopub.status.idle":"2025-05-03T18:19:11.694989Z","shell.execute_reply.started":"2025-05-03T18:19:11.677462Z","shell.execute_reply":"2025-05-03T18:19:11.694404Z"}},"outputs":[{"name":"stdout","text":"encoded: [15496, 11, 314, 716]\nencoded_tensor.shape: torch.Size([1, 4])\n","output_type":"stream"}],"execution_count":135},{"cell_type":"code","source":"model.eval() # disable dropout\n\nout = generate_text_simple(\n    model=model,\n    idx=encoded_tensor,\n    max_new_tokens=6,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\nprint(\"Output:\", out)\nprint(\"Output length:\", len(out[0]))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WoOfMpcsukAQ","outputId":"8b1a7bfd-18f1-449f-b924-f03a288db545","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:11.695688Z","iopub.execute_input":"2025-05-03T18:19:11.695934Z","iopub.status.idle":"2025-05-03T18:19:12.142976Z","shell.execute_reply.started":"2025-05-03T18:19:11.695913Z","shell.execute_reply":"2025-05-03T18:19:12.142230Z"}},"outputs":[{"name":"stdout","text":"Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\nOutput length: 10\n","output_type":"stream"}],"execution_count":136},{"cell_type":"code","source":"decoded_text = tokenizer.decode(out.squeeze(0).tolist())\nprint(decoded_text)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"udNF_28Oul8C","outputId":"9ad6aca7-a2dc-4ea0-d7c1-65b0749d44ef","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:12.143784Z","iopub.execute_input":"2025-05-03T18:19:12.144323Z","iopub.status.idle":"2025-05-03T18:19:12.148323Z","shell.execute_reply.started":"2025-05-03T18:19:12.144295Z","shell.execute_reply":"2025-05-03T18:19:12.147604Z"}},"outputs":[{"name":"stdout","text":"Hello, I am Featureiman Byeswickattribute argue\n","output_type":"stream"}],"execution_count":137},{"cell_type":"markdown","source":"\n\n---\n\n\n\n---\n\n","metadata":{"id":"cGcckdXkvAax"}},{"cell_type":"markdown","source":"**Chapter 5: Pretraining on Unlabeled Data**","metadata":{"id":"aoHAwTgUvCa9"}},{"cell_type":"code","source":"from importlib.metadata import version\n\npkgs = [\"matplotlib\",\n        \"numpy\",\n        \"tiktoken\",\n        \"torch\",\n        \"tensorflow\" # For OpenAI's pretrained weights\n       ]\nfor p in pkgs:\n    print(f\"{p} version: {version(p)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfoxArAyuqp9","outputId":"3f7b88eb-dffa-4a41-ed08-7098724711d3","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:12.148951Z","iopub.execute_input":"2025-05-03T18:19:12.149149Z","iopub.status.idle":"2025-05-03T18:19:12.170271Z","shell.execute_reply.started":"2025-05-03T18:19:12.149134Z","shell.execute_reply":"2025-05-03T18:19:12.169593Z"}},"outputs":[{"name":"stdout","text":"matplotlib version: 3.7.5\nnumpy version: 1.26.4\ntiktoken version: 0.9.0\ntorch version: 2.5.1+cu124\ntensorflow version: 2.18.0\n","output_type":"stream"}],"execution_count":138},{"cell_type":"markdown","source":"**5.1 Evaluating generative text models**","metadata":{"id":"K4VUXvKSvIsu"}},{"cell_type":"markdown","source":"**5.1.1 Using GPT to generate text**","metadata":{"id":"-9rfGEw_vKux"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJ5lYr5KeYXv","outputId":"a2d66fe7-7de9-4602-a760-c2154e7d1341","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:12.171354Z","iopub.execute_input":"2025-05-03T18:19:12.171613Z","iopub.status.idle":"2025-05-03T18:19:12.180593Z","shell.execute_reply.started":"2025-05-03T18:19:12.171585Z","shell.execute_reply":"2025-05-03T18:19:12.180048Z"}},"outputs":[],"execution_count":139},{"cell_type":"code","source":"# import sys\n# sys.path.append('/path/to/directory')\n","metadata":{"id":"v-aLKY_Teho9","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:12.181474Z","iopub.execute_input":"2025-05-03T18:19:12.181750Z","iopub.status.idle":"2025-05-03T18:19:12.195516Z","shell.execute_reply.started":"2025-05-03T18:19:12.181728Z","shell.execute_reply":"2025-05-03T18:19:12.194772Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"import previous_chapters","metadata":{"id":"82i9Uz4Nfywq","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:12.196378Z","iopub.execute_input":"2025-05-03T18:19:12.196662Z","iopub.status.idle":"2025-05-03T18:19:12.210485Z","shell.execute_reply.started":"2025-05-03T18:19:12.196636Z","shell.execute_reply":"2025-05-03T18:19:12.209696Z"}},"outputs":[],"execution_count":141},{"cell_type":"code","source":"import torch\nfrom previous_chapters import GPTModel\n\nGPT_CONFIG_124M = {\n    \"vocab_size\": 50257,   # Vocabulary size\n    \"context_length\": 256, # Shortened context length (orig: 1024)\n    \"emb_dim\": 768,        # Embedding dimension\n    \"n_heads\": 12,         # Number of attention heads\n    \"n_layers\": 12,        # Number of layers\n    \"drop_rate\": 0.1,      # Dropout rate\n    \"qkv_bias\": False      # Query-key-value bias\n}\n\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval();  # Disable dropout during inference","metadata":{"id":"-s-ExOsevFvO","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:12.211376Z","iopub.execute_input":"2025-05-03T18:19:12.211600Z","iopub.status.idle":"2025-05-03T18:19:13.652889Z","shell.execute_reply.started":"2025-05-03T18:19:12.211583Z","shell.execute_reply":"2025-05-03T18:19:13.652093Z"}},"outputs":[],"execution_count":142},{"cell_type":"code","source":"import tiktoken\nfrom previous_chapters import generate_text_simple\n\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n\nstart_context = \"Every effort moves you\"\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=10,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"KmJMzY_RvP7D","outputId":"9a6a7df9-e00b-4b26-b10d-9859295b2e60","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:13.653795Z","iopub.execute_input":"2025-05-03T18:19:13.653977Z","iopub.status.idle":"2025-05-03T18:19:14.420697Z","shell.execute_reply.started":"2025-05-03T18:19:13.653962Z","shell.execute_reply":"2025-05-03T18:19:14.419896Z"}},"outputs":[{"name":"stdout","text":"Output text:\n Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n","output_type":"stream"}],"execution_count":143},{"cell_type":"markdown","source":"**5.1.2 Calculating the text generation loss: cross-entropy and perplexity**","metadata":{"id":"F-WNay4OzYIW"}},{"cell_type":"code","source":"inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n                       [40,    1107, 588]])   #  \"I really like\"]\n\ntargets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n                        [1107,  588, 11311]]) #  \" really like chocolate\"]","metadata":{"id":"NhyEocU8zRxM","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.421569Z","iopub.execute_input":"2025-05-03T18:19:14.421820Z","iopub.status.idle":"2025-05-03T18:19:14.426153Z","shell.execute_reply.started":"2025-05-03T18:19:14.421795Z","shell.execute_reply":"2025-05-03T18:19:14.425434Z"}},"outputs":[],"execution_count":144},{"cell_type":"code","source":"with torch.no_grad():\n    logits = model(inputs)\n\nprobas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\nprint(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgsbKOemzZlO","outputId":"0d463e87-fb5e-41a9-dec9-f9b46d11a582","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.426916Z","iopub.execute_input":"2025-05-03T18:19:14.427227Z","iopub.status.idle":"2025-05-03T18:19:14.531233Z","shell.execute_reply.started":"2025-05-03T18:19:14.427202Z","shell.execute_reply":"2025-05-03T18:19:14.530568Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 3, 50257])\n","output_type":"stream"}],"execution_count":145},{"cell_type":"code","source":"token_ids = torch.argmax(probas, dim=-1, keepdim=True)\nprint(\"Token IDs:\\n\", token_ids)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjXplBjize7w","outputId":"953d91fc-957a-47a7-f87f-8086d748e476","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.531966Z","iopub.execute_input":"2025-05-03T18:19:14.532659Z","iopub.status.idle":"2025-05-03T18:19:14.537779Z","shell.execute_reply.started":"2025-05-03T18:19:14.532635Z","shell.execute_reply":"2025-05-03T18:19:14.537057Z"}},"outputs":[{"name":"stdout","text":"Token IDs:\n tensor([[[16657],\n         [  339],\n         [42826]],\n\n        [[49906],\n         [29669],\n         [41751]]])\n","output_type":"stream"}],"execution_count":146},{"cell_type":"code","source":"# Make sure this function is defined and run first\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) if token_ids.dim() == 2 else token_ids  # handles both with and without batch\n    return tokenizer.decode(flat.tolist())\n\n# Example usage (after running the function definition above)\nprint(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\nprint(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":351},"id":"ebNy2n2pzhih","outputId":"2b561233-ae5e-450c-f38e-1c3bd40c66d2","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.538609Z","iopub.execute_input":"2025-05-03T18:19:14.538856Z","iopub.status.idle":"2025-05-03T18:19:14.551018Z","shell.execute_reply.started":"2025-05-03T18:19:14.538834Z","shell.execute_reply":"2025-05-03T18:19:14.550439Z"}},"outputs":[{"name":"stdout","text":"Targets batch 1:  effort moves you\nOutputs batch 1:  Armed heNetflix\n","output_type":"stream"}],"execution_count":147},{"cell_type":"code","source":"text_idx = 0\ntarget_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\nprint(\"Text 1:\", target_probas_1)\n\ntext_idx = 1\ntarget_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\nprint(\"Text 2:\", target_probas_2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttrOvp3Lzjhw","outputId":"32e3d018-a7dd-4218-de4c-97a7e3cf4064","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.551905Z","iopub.execute_input":"2025-05-03T18:19:14.552173Z","iopub.status.idle":"2025-05-03T18:19:14.570724Z","shell.execute_reply.started":"2025-05-03T18:19:14.552150Z","shell.execute_reply":"2025-05-03T18:19:14.570161Z"}},"outputs":[{"name":"stdout","text":"Text 1: tensor([    0.0001,     0.0000,     0.0000])\nText 2: tensor([    0.0000,     0.0001,     0.0000])\n","output_type":"stream"}],"execution_count":148},{"cell_type":"code","source":"# Compute logarithm of all token probabilities\nlog_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\nprint(log_probas)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qaIOq3M9znrV","outputId":"f4e13f97-c8d7-4388-9da4-6f483b96fd50","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.571411Z","iopub.execute_input":"2025-05-03T18:19:14.571672Z","iopub.status.idle":"2025-05-03T18:19:14.581381Z","shell.execute_reply.started":"2025-05-03T18:19:14.571643Z","shell.execute_reply":"2025-05-03T18:19:14.580816Z"}},"outputs":[{"name":"stdout","text":"tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n","output_type":"stream"}],"execution_count":149},{"cell_type":"code","source":"# Calculate the average probability for each token\navg_log_probas = torch.mean(log_probas)\nprint(avg_log_probas)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FrDHP2yezpfg","outputId":"9860d3b3-3148-47f7-94ba-9d43ae6289c3","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.582080Z","iopub.execute_input":"2025-05-03T18:19:14.582345Z","iopub.status.idle":"2025-05-03T18:19:14.596227Z","shell.execute_reply.started":"2025-05-03T18:19:14.582322Z","shell.execute_reply":"2025-05-03T18:19:14.595662Z"}},"outputs":[{"name":"stdout","text":"tensor(-10.7940)\n","output_type":"stream"}],"execution_count":150},{"cell_type":"code","source":"neg_avg_log_probas = avg_log_probas * -1\nprint(neg_avg_log_probas)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-B1FXwxBzrVW","outputId":"8ab35c0f-741c-4e44-f596-ceb6466aaaf9","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.596928Z","iopub.execute_input":"2025-05-03T18:19:14.597136Z","iopub.status.idle":"2025-05-03T18:19:14.610177Z","shell.execute_reply.started":"2025-05-03T18:19:14.597095Z","shell.execute_reply":"2025-05-03T18:19:14.609477Z"}},"outputs":[{"name":"stdout","text":"tensor(10.7940)\n","output_type":"stream"}],"execution_count":151},{"cell_type":"code","source":"# Logits have shape (batch_size, num_tokens, vocab_size)\nprint(\"Logits shape:\", logits.shape)\n\n# Targets have shape (batch_size, num_tokens)\nprint(\"Targets shape:\", targets.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LwATXeILztNg","outputId":"45fccfd7-dc5d-4dc6-904b-344217a1ec1e","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.610859Z","iopub.execute_input":"2025-05-03T18:19:14.611066Z","iopub.status.idle":"2025-05-03T18:19:14.623712Z","shell.execute_reply.started":"2025-05-03T18:19:14.611043Z","shell.execute_reply":"2025-05-03T18:19:14.623179Z"}},"outputs":[{"name":"stdout","text":"Logits shape: torch.Size([2, 3, 50257])\nTargets shape: torch.Size([2, 3])\n","output_type":"stream"}],"execution_count":152},{"cell_type":"code","source":"logits_flat = logits.flatten(0, 1)\ntargets_flat = targets.flatten()\n\nprint(\"Flattened logits:\", logits_flat.shape)\nprint(\"Flattened targets:\", targets_flat.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GSv3zjbzvBL","outputId":"5c3c6469-73fd-480a-9768-779fcd2311aa","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.624407Z","iopub.execute_input":"2025-05-03T18:19:14.624614Z","iopub.status.idle":"2025-05-03T18:19:14.639486Z","shell.execute_reply.started":"2025-05-03T18:19:14.624598Z","shell.execute_reply":"2025-05-03T18:19:14.638724Z"}},"outputs":[{"name":"stdout","text":"Flattened logits: torch.Size([6, 50257])\nFlattened targets: torch.Size([6])\n","output_type":"stream"}],"execution_count":153},{"cell_type":"code","source":"loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\nprint(loss)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-oWk0IBzxIx","outputId":"ff36526c-5d08-4f5e-9e03-e109b5224173","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.640079Z","iopub.execute_input":"2025-05-03T18:19:14.640377Z","iopub.status.idle":"2025-05-03T18:19:14.657840Z","shell.execute_reply.started":"2025-05-03T18:19:14.640342Z","shell.execute_reply":"2025-05-03T18:19:14.657230Z"}},"outputs":[{"name":"stdout","text":"tensor(10.7940)\n","output_type":"stream"}],"execution_count":154},{"cell_type":"code","source":"perplexity = torch.exp(loss)\nprint(perplexity)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnZwprCvzzYw","outputId":"8c570d1e-425d-4cb6-bc9b-e1a8640027b2","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.658684Z","iopub.execute_input":"2025-05-03T18:19:14.658959Z","iopub.status.idle":"2025-05-03T18:19:14.669857Z","shell.execute_reply.started":"2025-05-03T18:19:14.658937Z","shell.execute_reply":"2025-05-03T18:19:14.669168Z"}},"outputs":[{"name":"stdout","text":"tensor(48725.8203)\n","output_type":"stream"}],"execution_count":155},{"cell_type":"markdown","source":"**5.1.3 Calculating the training and validation set losses**","metadata":{"id":"8aRrZkRYz6E0"}},{"cell_type":"code","source":"import os\nimport urllib.request\n\nfile_path = \"the-verdict.txt\"\nurl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n\nif not os.path.exists(file_path):\n    with urllib.request.urlopen(url) as response:\n        text_data = response.read().decode('utf-8')\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(text_data)\nelse:\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text_data = file.read()","metadata":{"id":"1iS5tBV8z2DW","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.670642Z","iopub.execute_input":"2025-05-03T18:19:14.670834Z","iopub.status.idle":"2025-05-03T18:19:14.681903Z","shell.execute_reply.started":"2025-05-03T18:19:14.670819Z","shell.execute_reply":"2025-05-03T18:19:14.681392Z"}},"outputs":[],"execution_count":156},{"cell_type":"code","source":"# First 100 characters\nprint(text_data[:99])","metadata":{"id":"5detOsuXz9O1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9714d5fb-7279-4b0a-ee81-f2dc303df34d","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.682572Z","iopub.execute_input":"2025-05-03T18:19:14.682794Z","iopub.status.idle":"2025-05-03T18:19:14.696093Z","shell.execute_reply.started":"2025-05-03T18:19:14.682773Z","shell.execute_reply":"2025-05-03T18:19:14.695453Z"}},"outputs":[{"name":"stdout","text":"I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n","output_type":"stream"}],"execution_count":157},{"cell_type":"code","source":"# Last 100 characters\nprint(text_data[-99:])","metadata":{"id":"fLFXBQ2nz_aB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"124d525c-c00b-4447-93a8-5f47f5354672","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.696929Z","iopub.execute_input":"2025-05-03T18:19:14.697267Z","iopub.status.idle":"2025-05-03T18:19:14.714045Z","shell.execute_reply.started":"2025-05-03T18:19:14.697242Z","shell.execute_reply":"2025-05-03T18:19:14.713366Z"}},"outputs":[{"name":"stdout","text":"it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n","output_type":"stream"}],"execution_count":158},{"cell_type":"code","source":"total_characters = len(text_data)\ntotal_tokens = len(tokenizer.encode(text_data))\n\nprint(\"Characters:\", total_characters)\nprint(\"Tokens:\", total_tokens)","metadata":{"id":"Vu9cFJBn0DqV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"719f5985-855a-47e7-e6e9-eaeadd75c793","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.714865Z","iopub.execute_input":"2025-05-03T18:19:14.715095Z","iopub.status.idle":"2025-05-03T18:19:14.730900Z","shell.execute_reply.started":"2025-05-03T18:19:14.715080Z","shell.execute_reply":"2025-05-03T18:19:14.730284Z"}},"outputs":[{"name":"stdout","text":"Characters: 20479\nTokens: 5145\n","output_type":"stream"}],"execution_count":159},{"cell_type":"code","source":"# import os\n\n# # Paths\n# working_dir = \"/kaggle/working/\"\n# old_name = os.path.join(working_dir, \"previous_chapters (1).py\")\n# new_name = os.path.join(working_dir, \"previous_chapters.py\")\n\n# # Rename if the file exists\n# if os.path.exists(old_name):\n#     os.rename(old_name, new_name)\n#     print(f\"Renamed: {old_name} -> {new_name}\")\n# else:\n#     print(\"File 'previous_chapters (1).py' not found in /kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.731509Z","iopub.execute_input":"2025-05-03T18:19:14.731688Z","iopub.status.idle":"2025-05-03T18:19:14.741606Z","shell.execute_reply.started":"2025-05-03T18:19:14.731673Z","shell.execute_reply":"2025-05-03T18:19:14.740942Z"}},"outputs":[],"execution_count":160},{"cell_type":"code","source":"from previous_chapters_1 import create_dataloader_v1\n# Alternatively:\n# from llms_from_scratch.ch02 import create_dataloader_v1\n\n# Train/validation ratio\ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntrain_data = text_data[:split_idx]\nval_data = text_data[split_idx:]\n\n\ntorch.manual_seed(123)\n\ntrain_loader = create_dataloader_v1(\n    train_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=True,\n    shuffle=True,\n    num_workers=0\n)\n\nval_loader = create_dataloader_v1(\n    val_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=GPT_CONFIG_124M[\"context_length\"],\n    drop_last=False,\n    shuffle=False,\n    num_workers=0\n)","metadata":{"id":"5P3T9sJ00IRD","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.742337Z","iopub.execute_input":"2025-05-03T18:19:14.742646Z","iopub.status.idle":"2025-05-03T18:19:14.766636Z","shell.execute_reply.started":"2025-05-03T18:19:14.742623Z","shell.execute_reply":"2025-05-03T18:19:14.765910Z"}},"outputs":[],"execution_count":161},{"cell_type":"code","source":"# Sanity check\n\nif total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n    print(\"Not enough tokens for the training loader. \"\n          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n          \"increase the `training_ratio`\")\n\nif total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n    print(\"Not enough tokens for the validation loader. \"\n          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n          \"decrease the `training_ratio`\")","metadata":{"id":"MPghYo1W0Lco","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.767402Z","iopub.execute_input":"2025-05-03T18:19:14.767633Z","iopub.status.idle":"2025-05-03T18:19:14.772274Z","shell.execute_reply.started":"2025-05-03T18:19:14.767616Z","shell.execute_reply":"2025-05-03T18:19:14.771681Z"}},"outputs":[],"execution_count":162},{"cell_type":"code","source":"print(\"Train loader:\")\nfor x, y in train_loader:\n    print(x.shape, y.shape)\n\nprint(\"\\nValidation loader:\")\nfor x, y in val_loader:\n    print(x.shape, y.shape)","metadata":{"id":"-uW4YRd30N3i","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ff8a1b21-abb4-4b96-f57b-72e4abf40ce2","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.772981Z","iopub.execute_input":"2025-05-03T18:19:14.773229Z","iopub.status.idle":"2025-05-03T18:19:14.788265Z","shell.execute_reply.started":"2025-05-03T18:19:14.773202Z","shell.execute_reply":"2025-05-03T18:19:14.787570Z"}},"outputs":[{"name":"stdout","text":"Train loader:\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\n\nValidation loader:\ntorch.Size([2, 256]) torch.Size([2, 256])\n","output_type":"stream"}],"execution_count":163},{"cell_type":"code","source":"train_tokens = 0\nfor input_batch, target_batch in train_loader:\n    train_tokens += input_batch.numel()\n\nval_tokens = 0\nfor input_batch, target_batch in val_loader:\n    val_tokens += input_batch.numel()\n\nprint(\"Training tokens:\", train_tokens)\nprint(\"Validation tokens:\", val_tokens)\nprint(\"All tokens:\", train_tokens + val_tokens)","metadata":{"id":"L1HCjtxR0QEn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ebf00c0c-2fed-416c-d64a-34b5e8d0c763","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.788857Z","iopub.execute_input":"2025-05-03T18:19:14.789078Z","iopub.status.idle":"2025-05-03T18:19:14.805773Z","shell.execute_reply.started":"2025-05-03T18:19:14.789053Z","shell.execute_reply":"2025-05-03T18:19:14.805236Z"}},"outputs":[{"name":"stdout","text":"Training tokens: 4608\nValidation tokens: 512\nAll tokens: 5120\n","output_type":"stream"}],"execution_count":164},{"cell_type":"code","source":"def calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n    return loss\n\n\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if len(data_loader) == 0:\n        return float(\"nan\")\n    elif num_batches is None:\n        num_batches = len(data_loader)\n    else:\n        # Reduce the number of batches to match the total number of batches in the data loader\n        # if num_batches exceeds the number of batches in the data loader\n        num_batches = min(num_batches, len(data_loader))\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item()\n        else:\n            break\n    return total_loss / num_batches","metadata":{"id":"wsxE0J0e0SfW","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.806485Z","iopub.execute_input":"2025-05-03T18:19:14.806704Z","iopub.status.idle":"2025-05-03T18:19:14.816836Z","shell.execute_reply.started":"2025-05-03T18:19:14.806680Z","shell.execute_reply":"2025-05-03T18:19:14.816271Z"}},"outputs":[],"execution_count":165},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Note:\n# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n# However, the resulting loss values may be slightly different.\n\n#if torch.cuda.is_available():\n#    device = torch.device(\"cuda\")\n#elif torch.backends.mps.is_available():\n#    device = torch.device(\"mps\")\n#else:\n#    device = torch.device(\"cpu\")\n#\n# print(f\"Using {device} device.\")\n\n\nmodel.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n\n\ntorch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n\nwith torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n    train_loss = calc_loss_loader(train_loader, model, device)\n    val_loss = calc_loss_loader(val_loader, model, device)\n\nprint(\"Training loss:\", train_loss)\nprint(\"Validation loss:\", val_loss)","metadata":{"id":"dqyyzwv60UmM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b105874-038d-4239-9e43-0d41e17fc5c2","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:14.817421Z","iopub.execute_input":"2025-05-03T18:19:14.817594Z","iopub.status.idle":"2025-05-03T18:19:15.783098Z","shell.execute_reply.started":"2025-05-03T18:19:14.817580Z","shell.execute_reply":"2025-05-03T18:19:15.782475Z"}},"outputs":[{"name":"stdout","text":"Training loss: 10.987583690219456\nValidation loss: 10.981104850769043\n","output_type":"stream"}],"execution_count":166},{"cell_type":"markdown","source":"**5.2 Training an LLM**","metadata":{"id":"qK8QjnCs0bGM"}},{"cell_type":"code","source":"def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n                       eval_freq, eval_iter, start_context, tokenizer):\n    # Initialize lists to track losses and tokens seen\n    train_losses, val_losses, track_tokens_seen = [], [], []\n    tokens_seen, global_step = 0, -1\n\n    # Main training loop\n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward() # Calculate loss gradients\n            optimizer.step() # Update model weights using loss gradients\n            tokens_seen += input_batch.numel()\n            global_step += 1\n\n            # Optional evaluation step\n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader, device, eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n\n        # Print a sample text after each epoch\n        generate_and_print_sample(\n            model, tokenizer, device, start_context\n        )\n\n    return train_losses, val_losses, track_tokens_seen\n\n\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval()\n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\n\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n    context_size = model.pos_emb.weight.shape[0]\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n    with torch.no_grad():\n        token_ids = generate_text_simple(\n            model=model, idx=encoded,\n            max_new_tokens=50, context_size=context_size\n        )\n    decoded_text = token_ids_to_text(token_ids, tokenizer)\n    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n    model.train()","metadata":{"id":"atRqcWLn0XKk","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:15.783773Z","iopub.execute_input":"2025-05-03T18:19:15.783961Z","iopub.status.idle":"2025-05-03T18:19:15.792571Z","shell.execute_reply.started":"2025-05-03T18:19:15.783945Z","shell.execute_reply":"2025-05-03T18:19:15.791826Z"}},"outputs":[],"execution_count":167},{"cell_type":"code","source":"# Note:\n# Uncomment the following code to calculate the execution time\n# import time\n# start_time = time.time()\n\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n\nnum_epochs = 10\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model, train_loader, val_loader, optimizer, device,\n    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n    start_context=\"Every effort moves you\", tokenizer=tokenizer\n)\n\n# Note:\n# Uncomment the following code to show the execution time\n# end_time = time.time()\n# execution_time_minutes = (end_time - start_time) / 60\n# print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")","metadata":{"id":"msRsiIiI0fuY","colab":{"base_uri":"https://localhost:8080/","height":405},"outputId":"a9e6fee2-91de-4308-8987-34250e796020","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:15.793321Z","iopub.execute_input":"2025-05-03T18:19:15.793967Z","iopub.status.idle":"2025-05-03T18:19:38.718093Z","shell.execute_reply.started":"2025-05-03T18:19:15.793950Z","shell.execute_reply":"2025-05-03T18:19:38.717317Z"}},"outputs":[{"name":"stdout","text":"Ep 1 (Step 000000): Train loss 9.820, Val loss 9.933\nEp 1 (Step 000005): Train loss 8.065, Val loss 8.340\nEvery effort moves you,,,,,,,,,,,,.                                     \nEp 2 (Step 000010): Train loss 6.621, Val loss 7.052\nEp 2 (Step 000015): Train loss 6.048, Val loss 6.602\nEvery effort moves you, the,, and,,,,,,,,,.                                   \nEp 3 (Step 000020): Train loss 5.588, Val loss 6.480\nEp 3 (Step 000025): Train loss 5.554, Val loss 6.415\nEvery effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\nEp 4 (Step 000030): Train loss 5.174, Val loss 6.369\nEp 4 (Step 000035): Train loss 4.988, Val loss 6.376\nEvery effort moves you a a so a a a, and a-- the picture. Gisburn, and a was a--and, and a. Gisburn, and the of the of the of the of the of the a--I was a, and\nEp 5 (Step 000040): Train loss 4.360, Val loss 6.277\nEvery effort moves you, I had been, I had been, I had been a of the of the, I had been, I had been, I had been the of the of the picture, as of the honour of the of the picture of the of the of\nEp 6 (Step 000045): Train loss 4.080, Val loss 6.276\nEp 6 (Step 000050): Train loss 3.602, Val loss 6.215\nEvery effort moves you know the                                                \nEp 7 (Step 000055): Train loss 3.638, Val loss 6.189\nEp 7 (Step 000060): Train loss 2.839, Val loss 6.145\nEvery effort moves you know the fact, and I felt to the to the fact of the last word.                   \"I looked. I was dead? I was.   \nEp 8 (Step 000065): Train loss 2.420, Val loss 6.140\nEp 8 (Step 000070): Train loss 2.060, Val loss 6.190\nEvery effort moves you know,\" was not that, one of the deep arm-chairs forward. \"There: \"Yes, with the fact, as he had been the moment--as Jack himself, as he was his own of Jack's \"There were, as his\nEp 9 (Step 000075): Train loss 1.682, Val loss 6.210\nEp 9 (Step 000080): Train loss 1.331, Val loss 6.258\nEvery effort moves you know,\" was not that my hostess was \"interesting\": on the last word.    \"I looked, and to see a smile behind his pictures with a little him. \"Oh, I was \"There were days when I\nEp 10 (Step 000085): Train loss 1.035, Val loss 6.295\nEvery effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  \"I didn't dabble back his head to look up at the sketch of the donkey. \"There were days when I\n","output_type":"stream"}],"execution_count":168},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n\ndef plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n\n    # Plot training and validation loss against epochs\n    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.legend(loc=\"upper right\")\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n\n    # Create a second x-axis for tokens seen\n    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n    ax2.set_xlabel(\"Tokens seen\")\n\n    fig.tight_layout()  # Adjust layout to make room\n    plt.savefig(\"loss-plot.pdf\")\n    plt.show()\n\nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)","metadata":{"id":"Uzkbv_Np0kdz","colab":{"base_uri":"https://localhost:8080/","height":200},"outputId":"fa9380bc-01eb-4b82-a7aa-5f7b4c8a81d2","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:38.719446Z","iopub.execute_input":"2025-05-03T18:19:38.720428Z","iopub.status.idle":"2025-05-03T18:19:39.748346Z","shell.execute_reply.started":"2025-05-03T18:19:38.720407Z","shell.execute_reply":"2025-05-03T18:19:39.747588Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 500x300 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":169},{"cell_type":"markdown","source":"**5.3 Decoding strategies to control randomness**","metadata":{"id":"sC7Sbumq0ts9"}},{"cell_type":"markdown","source":"Inference is relatively cheap with a relatively small LLM as the GPT model we trained above, so there's no need to use a GPU for it in case you used a GPU for training it above\nUsing the generate_text_simple function (from the previous chapter) that we used earlier inside the simple training function, we can generate new text one word (or token) at a time","metadata":{}},{"cell_type":"code","source":"model.to(\"cpu\")\nmodel.eval()\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n    max_new_tokens=25,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"id":"e7bamI4A0n0I","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:39.749169Z","iopub.execute_input":"2025-05-03T18:19:39.749666Z","iopub.status.idle":"2025-05-03T18:19:43.061980Z","shell.execute_reply.started":"2025-05-03T18:19:39.749640Z","shell.execute_reply":"2025-05-03T18:19:43.061256Z"}},"outputs":[{"name":"stdout","text":"Output text:\n Every effort moves you?\"\n\n\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n\n\n","output_type":"stream"}],"execution_count":170},{"cell_type":"markdown","source":"**5.3.1 Temperature scaling**","metadata":{"id":"41k9t6FE0zk8"}},{"cell_type":"markdown","source":"Previously, we always sampled the token with the highest probability as the next token using torch.argmax\nTo add variety, we can sample the next token using The torch.multinomial(probs, num_samples=1), sampling from a probability distribution\nHere, each index's chance of being picked corresponds to its probability in the input tensor","metadata":{}},{"cell_type":"code","source":"vocab = {\n    \"closer\": 0,\n    \"every\": 1,\n    \"effort\": 2,\n    \"forward\": 3,\n    \"inches\": 4,\n    \"moves\": 5,\n    \"pizza\": 6,\n    \"toward\": 7,\n    \"you\": 8,\n}\n\ninverse_vocab = {v: k for k, v in vocab.items()}\n\n# Suppose input is \"every effort moves you\", and the LLM\n# returns the following logits for the next token:\nnext_token_logits = torch.tensor(\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n)\n\nprobas = torch.softmax(next_token_logits, dim=0)\nnext_token_id = torch.argmax(probas).item()\n\n# The next generated token is then as follows:\nprint(inverse_vocab[next_token_id])","metadata":{"id":"uNVNJKXG0xP_","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:43.062798Z","iopub.execute_input":"2025-05-03T18:19:43.063045Z","iopub.status.idle":"2025-05-03T18:19:43.069051Z","shell.execute_reply.started":"2025-05-03T18:19:43.063027Z","shell.execute_reply":"2025-05-03T18:19:43.068355Z"}},"outputs":[{"name":"stdout","text":"forward\n","output_type":"stream"}],"execution_count":171},{"cell_type":"code","source":"torch.manual_seed(123)\nnext_token_id = torch.multinomial(probas, num_samples=1).item()\nprint(inverse_vocab[next_token_id])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:19:43.069873Z","iopub.execute_input":"2025-05-03T18:19:43.070140Z","iopub.status.idle":"2025-05-03T18:19:43.097483Z","shell.execute_reply.started":"2025-05-03T18:19:43.070098Z","shell.execute_reply":"2025-05-03T18:19:43.096795Z"}},"outputs":[{"name":"stdout","text":"toward\n","output_type":"stream"}],"execution_count":172},{"cell_type":"code","source":"def print_sampled_tokens(probas):\n    torch.manual_seed(123) # Manual seed for reproducibility\n    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n    sampled_ids = torch.bincount(torch.tensor(sample), minlength=len(probas))\n    for i, freq in enumerate(sampled_ids):\n        print(f\"{freq} x {inverse_vocab[i]}\")\n\nprint_sampled_tokens(probas)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:20:57.153463Z","iopub.execute_input":"2025-05-03T18:20:57.153787Z","iopub.status.idle":"2025-05-03T18:20:57.197701Z","shell.execute_reply.started":"2025-05-03T18:20:57.153769Z","shell.execute_reply":"2025-05-03T18:20:57.196961Z"}},"outputs":[{"name":"stdout","text":"71 x closer\n2 x every\n0 x effort\n544 x forward\n2 x inches\n1 x moves\n0 x pizza\n376 x toward\n4 x you\n","output_type":"stream"}],"execution_count":173},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}